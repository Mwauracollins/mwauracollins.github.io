<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Optimizers | mwaura.AI</title>
<meta name=keywords content="optimization,deep-learning,gradient-descent"><meta name=description content="Diving into what makes AI 'smart'. "><meta name=author content="Mwaura Collins"><link rel=canonical href=https://mwauracollins.github.io/posts/2025-01-21-optimizers/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css rel=stylesheet><link rel=icon href=https://mwauracollins.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mwauracollins.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mwauracollins.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mwauracollins.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mwauracollins.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mwauracollins.github.io/posts/2025-01-21-optimizers/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://mwauracollins.github.io/posts/2025-01-21-optimizers/"><meta property="og:site_name" content="mwaura.AI"><meta property="og:title" content="Optimizers"><meta property="og:description" content="Diving into what makes AI 'smart'. "><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-21T12:34:55+03:00"><meta property="article:modified_time" content="2025-01-21T12:34:55+03:00"><meta property="article:tag" content="Optimization"><meta property="article:tag" content="Deep-Learning"><meta property="article:tag" content="Gradient-Descent"><meta name=twitter:card content="summary"><meta name=twitter:title content="Optimizers"><meta name=twitter:description content="Diving into what makes AI 'smart'. "><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://mwauracollins.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Optimizers","item":"https://mwauracollins.github.io/posts/2025-01-21-optimizers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Optimizers","name":"Optimizers","description":"Diving into what makes AI 'smart'. ","keywords":["optimization","deep-learning","gradient-descent"],"articleBody":"What are optimizers? Optimizers are mathematical functions or algorithms that aim to reduce the loss value of a model. In simpler terms, optimizers reduce the loss after forward propagation of a model which makes supervised models learn and become ‚Äòsmart‚Äô.\nWhen training a model, the objective is to minimize the loss/cost function by iteratively updating the model parameters(Weights and Biases) at each epoch.\nThis approach aims in reducing the loss by finding the global minima for the loss function.\nTo update the parameters or the learning algorithms we use the formula\n$$ W_{t+1} = W_{t} - \\eta \\cdot \\nabla L(W_{t}) $$Where:\n$ w_t $: Current weight at time $t$ $ \\eta $: Learning rate $ \\nabla L(w_t) $: Gradient of the loss function with respect to the weights. Choosing the right learning rate is crucial because it directly impacts how quickly and effectively a model learns:\nToo High: If the learning rate is too high, the model might ‚Äúovershoot‚Äù the optimal values, jumping past the minima. It might even cause the training process to diverge, leading to poor performance or failure to converge. Too Low: If the learning rate is too low, the model will converge very slowly. Training could take a long time and get stuck in sub-optimal minima because the updates are so tiny that it cannot explore enough of the parameter space to find the global minimum. The key is finding a balance: the learning rate should be large enough to make noticeable progress but small enough to avoid overshooting and becoming unstable.\nIn practice, learning rates are often adjusted dynamically using techniques like learning rate schedules or adaptive learning rates, such as those used in optimizers like Adam(adam), to fine-tune the training process.\nThere are different optimizers in ML and choosing the best optimizers depend on your application i.e. simpler models like a simple feedforward neural network will use a Stochastic Gradient Descent. and bigger models like Transformers using Adam or RMSProp\nThe optimizers discussed will be gradient-based optimizers for example:\nStochastic Gradient Descent AdaGrad RMSProp AdaDelta Adam (Adaptive Moment Estimation) NAG Non-gradient based optimizers like simulated annealing will not be covered.\nStochastic Gradient Descent The standard SGD involves calculating the gradient one data point at a time and the model‚Äôs weights are updated after each data point.\n$$ W_{t+1} = W_{t} - \\eta \\cdot \\nabla L(W_{t}) $$where:\n$ \\nabla L(W_{t}) $: Gradient of the loss function with respect to weights, computed for a single training sample. However, the updates can be noisy which leads to high variance in the weight changes and it is also slow to converge.\nBatch Gradient Descent It is the classic form of gradient descent. The gradient is computed using the entire dataset and the weights are updated after processing the entire dataset. This can be very slow if the dataset is very large.\n$$ W_{t+1} = W_{t} - \\eta \\cdot \\nabla L(W_{t}) $$ where:\n$ \\nabla L(W_{t}) $: is now the gradient computed over the entire dataset $X$. On the bright side it tends to have a smoother convergence.\nMini-batch Gradient Descent Here, we split the dataset into small slices - batches and calculate the gradient one mini-batch at a time. Updates are also done after processing each mini-batch\n$$ W_{t+1} = W_{t} - \\eta \\cdot \\nabla \\frac {1}{m} L(W_{t}; x^{(i)}, y^{(i)}) $$Where:\n$ m $: Number of samples in the mini-batch. $ x^{(i)}, y^{(i)} $: Input-output pairs in the mini-batch. $ \\nabla L(W_{t}; x^{(i)}, y^{(i)}) $: Gradient for the ùëñ-th sample in the mini-batch. This variant of GD is computationally efficient than SGD because it uses vectorized operations over batches and it also helps balance the trade-off between high variance of SGD and the slow convergence of Batch SGD.\nIts disadvantage is that it is a bit noisy.\nSGD with momentum The idea of noise has been addresses in gradient descent algorithms where the optimizers take a noisy path to reach the optimal minimum. This noisy approach tends to add computational costs as the number of iterations is increased.\nWhat momentum does is it helps in faster convergence of the loss function by adding the previous update to the current update making it a bit faster.\n$$ v_{t+1} = \\beta v_{t} + (1 - \\beta) \\nabla L (W_{t}) $$$$ W_{t+1} = W_{t} - \\eta \\cdot v_{t+1} $$ Without momentum: Imagine a ball on top of a hi;; If you let it just a roll, at any given instant, the motion depends on the steepness of the instant hill. The steeper the hill, the faster the ball goes; the shallower the hill, the slower the ball goes. In other words, the ball moves purely based on the current slope, or gradient, at every point.\nWith momentum: Now consider that the ball has some initial velocity, or in other words, momentum, when it starts rolling. The ball, while rolling, does not only respond to the present slope but also caries some of its previous speed. That means even when the slope flattens or reverses its direction, the ball would continue in the same direction for some time due to the built-up momentum. With time, it slows down; the ball would still be moving but would smoothly reach the bottom faster because it would not get stuck in shallow areas. This idea helps to avoid local minima and reduce the number of iterations to converge.\nLearning Rate decay Learning rate decay is a technique where the learning rate starts at a predefined value and gradually decreases over time as the model approaches convergence. This reduction in the learning rate helps to make smaller updates to the model‚Äôs parameters, allowing the optimization process to fine-tune the solution more precisely and gradually settle towards the global minimum.\nAdaGrad AdaGrad (Adaptive Gradient Algorithm) is a bit different compared to the others. It is designed to adapt the learning rate of each weight parameter based on its gradient. This helps improve performance for sparse data and leads to faster convergence in some cases.\nThe key idea behind Adagrad is to adapt the learning rate for each parameter, allowing it to take larger steps for parameters with smaller gradients and smaller steps for parameters with larger gradients. This adjustment happens throughout the training process.\nAdagrad keeps track of the sum of squared gradients for each parameter. This means parameters that have been updated frequently will have a larger accumulated gradient, leading to smaller learning rates in the future. The learning rate for each parameter is adjusted based on its historical gradient, so each parameter gets a tailored learning rate.\n$$ G_{t, i} = G_{t-1, i} + (\\nabla L(w_{t}, i))^2 $$$$ w_{t+1, i} = w_{t, i} - \\frac {\\eta}{\\sqrt{G_{t, i}} + \\epsilon} \\nabla L(w_{t}, i) $$where:\n$ G_{t, t} $ is the accumulated sum of squared gradients for parameter $ w_i $ at time step $t$ $\\epsilon$ is a small value set to avoid division by zero Because the learning rate adapts automatically, it reduces the need to finetune the learning rate.\nRMSProp RMSProp (Root Mean Square Propagation) is a variant of AdaGrad designed to solve some Adagrad‚Äôs tendecy to reduce the learning rate too aggressively over time.\nIt introduces a mechanism to ‚Äúforget‚Äù very old gradients by using an exponential moving average over the squared gradients instead of summing them all up like in AdaGrad. This makes it more robust and effective for non-stationary objectives and deep neural networks.\n$$ G_{t, i} = \\beta G_{t-1, i} + (1- \\beta) (\\nabla L(w_{t}, i))^2 $$$$ w_{t+1, i} = w_{t, i} - \\frac {\\eta}{\\sqrt{G_{t, i}} + \\epsilon} \\nabla L(w_{t}, i) $$where:\n$ \\beta $ is the decay rate(typically =0.9), controlling how much weight is given to the recent gradients vs past gradients. If you noticed, RMSProp can be thought of as AdaGrad with momentum (placing more emphasis on recent gradients).\nAdaDelta Like RMSProp, AdaDelta also seeks to address the issue of aggressive decay of the learning rate in AdaGrad. Instead of accumulating all the past squared gradients, AdaDelta limits the accumulation of a window of recent gradients using an exponential moving average similar to RMSProp.\nGradient Accumulation (Exponential Moving Average of Gradients):\n$$ E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta)(\\nabla L(w_t))^2 $$Update Accumulation (Exponential Moving Average of Updates):\n$$ E[\\Delta w^2]_t = \\beta E[\\Delta w^2]_{t-1} + (1 - \\beta)(\\Delta w_t)^2 $$Parameter Update:\n$$ \\Delta w_t = - \\frac{\\sqrt{E[\\Delta w^2]_{t-1} + \\epsilon}}{\\sqrt{E[g^2]_t + \\epsilon}} \\nabla L(w_t) $$Weight Update:\n$$ w_{t+1} = w_t + \\Delta w_t $$where:\n$ E[g^2]_t $: Exponential moving average of squared gradients at time step $ t $. $ E[\\Delta w^2]_t $: Moving average of past squared updates. $ \\beta $: Decay rate (typically $ \\beta = 0.9 $). $ \\sqrt{E[\\Delta w^2]_{t-1} + \\epsilon} $: Dynamically scaled update size. However, using AdaDelta is computationally expensive.\nAdam Adam (Adaptive Moment Estimation) is one of the most popular and widely used gradient based algorithms that combined the strengths of both momentum and RMSProp. It leverages on exponentially decaying averages of both past gradients (momentum) and their squared values (adaptive lr).\nThis means that instead of applying current gradients, we‚Äôre going to apply momentums like in the SGD optimizer with momentum, then apply a per-weight adaptive learning rate with the cache as done in RMSProp.\nAdam also includes bias correction terms to mitigate the biasness of the gradient and squared gradient towards 0.\nCompute Gradients:\nCalculate the gradient of the loss function:\n$$ \\nabla_\\theta L(\\theta) $$Update Biased Estimates:\nFirst moment (mean of gradients): $$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_\\theta L(\\theta) $$ Second moment (variance of gradients): $$ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\left( \\nabla_\\theta L(\\theta) \\right)^2 $$Bias Correction:\nCorrect for initial biases in $m_t$ and $v_t$:\n$$ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} $$Update Parameters:\nUse the corrected moments to adjust the weights:\n$$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t $$where:\n$\\beta_1$: Decay rate for the first moment. Normally set at 0.9. $\\beta_2$: Decay rate for the second moment (0.999) ","wordCount":"1664","inLanguage":"en","datePublished":"2025-01-21T12:34:55+03:00","dateModified":"2025-01-21T12:34:55+03:00","author":{"@type":"Person","name":"Mwaura Collins"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mwauracollins.github.io/posts/2025-01-21-optimizers/"},"publisher":{"@type":"Organization","name":"mwaura.AI","logo":{"@type":"ImageObject","url":"https://mwauracollins.github.io/favicon.ico"}}}</script><script>MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]]}}</script><script defer src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mwauracollins.github.io/ accesskey=h title="mwaura.AI (Alt + H)">mwaura.AI</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mwauracollins.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://mwauracollins.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://mwaura.tech/ title=mwaura.tech><span>mwaura.tech</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mwauracollins.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://mwauracollins.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Optimizers</h1><div class=post-description>Diving into what makes AI 'smart'.</div><div class=post-meta><span title='2025-01-21 12:34:55 +0300 EAT'>January 21, 2025</span>&nbsp;¬∑&nbsp;8 min&nbsp;¬∑&nbsp;1664 words&nbsp;¬∑&nbsp;Mwaura Collins</div></header><div class=post-content><h1 id=what-are-optimizers>What are optimizers?<a hidden class=anchor aria-hidden=true href=#what-are-optimizers>#</a></h1><p>Optimizers are mathematical functions or algorithms that aim to reduce the loss value of a model.
In simpler terms, optimizers reduce the loss after forward propagation of a model which makes supervised models learn and become &lsquo;smart&rsquo;.</p><p>When training a model, the objective is to minimize the loss/cost function by iteratively updating the model parameters(Weights and Biases) at each epoch.</p><p>This approach aims in reducing the loss by finding the global minima for the loss function.</p><p><img loading=lazy src=/images/optimizers/minima.png></p><p>To update the parameters or the learning algorithms we use the formula</p>$$
W_{t+1} = W_{t} - \eta \cdot \nabla L(W_{t})
$$<p>Where:</p><ul><li>$ w_t $: Current weight at time $t$</li><li>$ \eta $: Learning rate</li><li>$ \nabla L(w_t) $: Gradient of the loss function with
respect to the weights.</li></ul><hr><p>Choosing the <strong>right learning rate</strong> is crucial because it directly impacts how quickly and effectively a model learns:</p><ul><li><strong>Too High</strong>: If the learning rate is too high, the model might &ldquo;overshoot&rdquo; the optimal values, jumping past the minima. It might even cause the training process to diverge, leading to poor performance or failure to converge.</li></ul><p><img loading=lazy src=/images/optimizers/high-lr.png></p><ul><li><strong>Too Low</strong>: If the learning rate is too low, the model will converge very slowly. Training could take a long time and get stuck in sub-optimal minima because the updates are so tiny that it cannot explore enough of the parameter space to find the global minimum.</li></ul><p><img loading=lazy src=/images/optimizers/small-lr.png></p><p>The key is finding a balance: the learning rate should be large enough to make noticeable progress but small enough to avoid overshooting and becoming unstable.</p><p>In practice, learning rates are often adjusted dynamically using techniques like <em>learning rate schedules</em> or <em>adaptive learning rates</em>, such as those used in optimizers like <strong>Adam</strong>(adam), to fine-tune the training process.</p><hr><p>There are different optimizers in ML and choosing the best optimizers depend on your application i.e. simpler models like a simple feedforward neural network will use a <a href=#stochastic-gradient-descent>Stochastic Gradient Descent</a>. and bigger models like Transformers using <a href=#adam>Adam</a> or <a href=#rmsprop>RMSProp</a></p><p>The optimizers discussed will be gradient-based optimizers for example:</p><ul><li><a href=#stochastic-gradient-descent>Stochastic Gradient Descent</a></li><li><a href=#adagrad>AdaGrad</a></li><li><a href=#rmsprop>RMSProp</a></li><li><a href=#adadelta>AdaDelta</a></li><li><a href=#adam-adaptive-moment-estimation>Adam (Adaptive Moment Estimation)</a></li><li>NAG</li></ul><p>Non-gradient based optimizers like simulated annealing will not be covered.</p><h2 id=stochastic-gradient-descent>Stochastic Gradient Descent<a hidden class=anchor aria-hidden=true href=#stochastic-gradient-descent>#</a></h2><p>The standard SGD involves calculating the gradient one data point at a time and the model&rsquo;s weights are updated after each data point.</p>$$
W_{t+1} = W_{t} - \eta \cdot \nabla L(W_{t})
$$<p>where:</p><ul><li>$ \nabla L(W_{t}) $: Gradient of the loss function with respect to weights, computed for a single training sample.</li></ul><p>However, the updates can be noisy which leads to high variance in the weight changes and it is also slow to converge.</p><h3 id=batch-gradient-descent>Batch Gradient Descent<a hidden class=anchor aria-hidden=true href=#batch-gradient-descent>#</a></h3><p>It is the classic form of gradient descent.
The gradient is computed using the entire dataset and the weights are updated after processing the entire dataset. This can be very slow if the dataset is very large.</p>$$
W_{t+1} = W_{t} - \eta \cdot \nabla L(W_{t})
$$<p>where:</p><ul><li>$ \nabla L(W_{t}) $: is now the gradient computed over the entire dataset $X$.</li></ul><p>On the bright side it tends to have a smoother convergence.</p><h3 id=mini-batch-gradient-descent>Mini-batch Gradient Descent<a hidden class=anchor aria-hidden=true href=#mini-batch-gradient-descent>#</a></h3><p>Here, we split the dataset into small slices - batches and calculate the gradient one mini-batch at a time. Updates are also done after processing each mini-batch</p>$$
W_{t+1} = W_{t} - \eta \cdot \nabla \frac {1}{m} L(W_{t}; x^{(i)}, y^{(i)})
$$<p>Where:</p><ul><li>$ m $: Number of samples in the mini-batch.</li><li>$ x^{(i)}, y^{(i)} $: Input-output pairs in the mini-batch.</li><li>$ \nabla L(W_{t}; x^{(i)}, y^{(i)}) $: Gradient for the ùëñ-th sample in the mini-batch.</li></ul><p>This variant of GD is computationally efficient than SGD because it uses vectorized operations over batches and it also helps balance the trade-off between high variance of SGD and the slow convergence of Batch SGD.</p><p>Its disadvantage is that it is a bit noisy.</p><h3 id=sgd-with-momentum>SGD with momentum<a hidden class=anchor aria-hidden=true href=#sgd-with-momentum>#</a></h3><p>The idea of noise has been addresses in gradient descent algorithms where the optimizers take a noisy path to reach the optimal minimum. This noisy approach tends to add computational costs as the number of iterations is increased.</p><p>What momentum does is it helps in faster convergence of the loss function by adding the previous update to the current update making it a bit faster.</p>$$
v_{t+1} = \beta v_{t} + (1 - \beta) \nabla L (W_{t})
$$$$
W_{t+1} = W_{t} - \eta \cdot v_{t+1}
$$<hr><ul><li><p><strong>Without momentum:</strong> Imagine a ball on top of a hi;; If you let it just a roll, at any given instant, the motion depends on the steepness of the instant hill. The steeper the hill, the faster the ball goes; the shallower the hill, the slower the ball goes. In other words, the ball moves purely based on the current slope, or gradient, at every point.</p></li><li><p><strong>With momentum:</strong> Now consider that the ball has some initial velocity, or in other words, momentum, when it starts rolling. The ball, while rolling, does not only respond to the present slope but also caries some of its previous speed. That means even when the slope flattens or reverses its direction, the ball would continue in the same direction for some time due to the built-up momentum. With time, it slows down; the ball would still be moving but would smoothly reach the bottom faster because it would not get stuck in shallow areas. This idea helps to avoid local minima and reduce the number of iterations to converge.</p></li></ul><hr><h3 id=learning-rate-decay>Learning Rate decay<a hidden class=anchor aria-hidden=true href=#learning-rate-decay>#</a></h3><p>Learning rate decay is a technique where the learning rate starts at a predefined value and gradually decreases over time as the model approaches convergence. This reduction in the learning rate helps to make smaller updates to the model&rsquo;s parameters, allowing the optimization process to fine-tune the solution more precisely and gradually settle towards the global minimum.</p><h2 id=adagrad>AdaGrad<a hidden class=anchor aria-hidden=true href=#adagrad>#</a></h2><p>AdaGrad (Adaptive Gradient Algorithm) is a bit different compared to the others. It is designed to adapt the learning rate of each weight parameter based on its gradient. This helps improve performance for sparse data and leads to faster convergence in some cases.</p><p>The key idea behind Adagrad is to adapt the learning rate for each parameter, allowing it to take larger steps for parameters with smaller gradients and smaller steps for parameters with larger gradients. This adjustment happens throughout the training process.</p><p>Adagrad keeps track of the sum of squared gradients for each parameter. This means parameters that have been updated frequently will have a larger accumulated gradient, leading to smaller learning rates in the future. The learning rate for each parameter is adjusted based on its historical gradient, so each parameter gets a tailored learning rate.</p>$$
G_{t, i} = G_{t-1, i} + (\nabla L(w_{t}, i))^2
$$$$
w_{t+1, i} = w_{t, i} - \frac {\eta}{\sqrt{G_{t, i}} + \epsilon} \nabla L(w_{t}, i)
$$<p>where:</p><ul><li>$ G_{t, t} $ is the accumulated sum of squared gradients for parameter $ w_i $ at time step $t$</li><li>$\epsilon$ is a small value set to avoid division by zero</li></ul><p>Because the learning rate adapts automatically, it reduces the need to finetune the learning rate.</p><h2 id=rmsprop>RMSProp<a hidden class=anchor aria-hidden=true href=#rmsprop>#</a></h2><p>RMSProp (Root Mean Square Propagation) is a variant of AdaGrad designed to solve some Adagrad&rsquo;s tendecy to reduce the learning rate too aggressively over time.</p><p>It introduces a mechanism to &ldquo;forget&rdquo; very old gradients by using an exponential moving average over the squared gradients instead of summing them all up like in AdaGrad. This makes it more robust and effective for non-stationary objectives and deep neural networks.</p>$$
G_{t, i} = \beta G_{t-1, i} + (1- \beta) (\nabla L(w_{t}, i))^2
$$$$
w_{t+1, i} = w_{t, i} - \frac {\eta}{\sqrt{G_{t, i}} + \epsilon} \nabla L(w_{t}, i)
$$<p>where:</p><ul><li>$ \beta $ is the decay rate(typically =0.9), controlling how much weight is given to the recent gradients vs past gradients.</li></ul><p>If you noticed, RMSProp can be thought of as AdaGrad with momentum (placing more emphasis on recent gradients).</p><h2 id=adadelta>AdaDelta<a hidden class=anchor aria-hidden=true href=#adadelta>#</a></h2><p>Like RMSProp, AdaDelta also seeks to address the issue of aggressive decay of the learning rate in AdaGrad.
Instead of accumulating all the past squared gradients, AdaDelta limits the accumulation of a window of recent gradients using an exponential moving average similar to RMSProp.</p><p><strong>Gradient Accumulation</strong> (Exponential Moving Average of Gradients):</p>$$
E[g^2]_t = \beta E[g^2]_{t-1} + (1 - \beta)(\nabla L(w_t))^2
$$<p><strong>Update Accumulation</strong> (Exponential Moving Average of Updates):</p>$$
E[\Delta w^2]_t = \beta E[\Delta w^2]_{t-1} + (1 - \beta)(\Delta w_t)^2
$$<p><strong>Parameter Update</strong>:</p>$$
\Delta w_t = - \frac{\sqrt{E[\Delta w^2]_{t-1} + \epsilon}}{\sqrt{E[g^2]_t + \epsilon}} \nabla L(w_t)
$$<p><strong>Weight Update</strong>:</p>$$
w_{t+1} = w_t + \Delta w_t
$$<p>where:</p><ul><li>$ E[g^2]_t $: Exponential moving average of squared gradients at time step $ t $.</li><li>$ E[\Delta w^2]_t $: Moving average of past squared updates.</li><li>$ \beta $: Decay rate (typically $ \beta = 0.9 $).</li><li>$ \sqrt{E[\Delta w^2]_{t-1} + \epsilon} $: Dynamically scaled update size.</li></ul><p>However, using AdaDelta is computationally expensive.</p><h2 id=adam>Adam<a hidden class=anchor aria-hidden=true href=#adam>#</a></h2><p>Adam (Adaptive Moment Estimation) is one of the most popular and widely used gradient based algorithms that combined the strengths of both momentum and RMSProp.
It leverages on exponentially decaying averages of both past gradients (momentum) and their squared values (adaptive lr).</p><p>This means that instead of applying current gradients, we‚Äôre going to apply momentums like in the SGD optimizer with momentum, then apply a per-weight adaptive learning rate with the cache as done in RMSProp.</p><p>Adam also includes bias correction terms to mitigate the biasness of the gradient and squared gradient towards 0.</p><p><strong>Compute Gradients</strong>:<br>Calculate the gradient of the loss function:</p>$$
\nabla_\theta L(\theta)
$$<p><strong>Update Biased Estimates</strong>:</p><ul><li>First moment (mean of gradients):</li></ul>$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_\theta L(\theta)
$$<ul><li>Second moment (variance of gradients):</li></ul>$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) \left( \nabla_\theta L(\theta) \right)^2
$$<p><strong>Bias Correction</strong>:<br>Correct for initial biases in $m_t$ and $v_t$:</p>$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$<p><strong>Update Parameters</strong>:<br>Use the corrected moments to adjust the weights:</p>$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$<p>where:</p><ul><li>$\beta_1$: Decay rate for the first moment. Normally set at 0.9.</li><li>$\beta_2$: Decay rate for the second moment (0.999)</li></ul><p><img loading=lazy src=https://editor.analyticsvidhya.com/uploads/121381obtV.gif>
<img loading=lazy src=https://miro.medium.com/v2/resize:fit:640/format:webp/1*XVFmo9NxLnwDr3SxzKy-rA.gif></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://mwauracollins.github.io/tags/optimization/>Optimization</a></li><li><a href=https://mwauracollins.github.io/tags/deep-learning/>Deep-Learning</a></li><li><a href=https://mwauracollins.github.io/tags/gradient-descent/>Gradient-Descent</a></li></ul><nav class=paginav><a class=next href=https://mwauracollins.github.io/posts/2025-01-16-autoencoders/><span class=title>Next ¬ª</span><br><span>Autoencoders</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Optimizers on x" href="https://x.com/intent/tweet/?text=Optimizers&amp;url=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-01-21-optimizers%2f&amp;hashtags=optimization%2cdeep-learning%2cgradient-descent"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Optimizers on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-01-21-optimizers%2f&amp;title=Optimizers&amp;summary=Optimizers&amp;source=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-01-21-optimizers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Optimizers on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-01-21-optimizers%2f&title=Optimizers"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Optimizers on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-01-21-optimizers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Optimizers on whatsapp" href="https://api.whatsapp.com/send?text=Optimizers%20-%20https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-01-21-optimizers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Optimizers on telegram" href="https://telegram.me/share/url?text=Optimizers&amp;url=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-01-21-optimizers%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Optimizers on ycombinator" href="https://news.ycombinator.com/submitlink?t=Optimizers&u=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-01-21-optimizers%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://mwauracollins.github.io/>mwaura.AI</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>