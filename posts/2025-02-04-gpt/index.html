<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Rolling Your Own GPT | mwaura.AI</title>
<meta name=keywords content="LLMs,Transformer,GPT"><meta name=description content="Training a GPT model sounds like a moonshot but it is actually just a series of simple and well-defined steps.
At its core, it is just a giant text predictor, fed with tons of data and then allowed to guess the next word. The real challenge is not training it to predict the next word, but to make it produce something useful like answers to your assignment due midnight.üòÖ
You need some special type of training like Reinforcement Learning with Human Feedback (RLHF). Though you can get good responses without RLHF, RLHF is required to make it chatbot-like."><meta name=author content="Mwaura Collins"><link rel=canonical href=https://mwauracollins.github.io/posts/2025-02-04-gpt/><meta name=google-site-verification content="G-S66PW9R1B3"><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css rel=stylesheet><link rel=icon href=https://mwauracollins.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mwauracollins.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mwauracollins.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mwauracollins.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mwauracollins.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mwauracollins.github.io/posts/2025-02-04-gpt/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-S66PW9R1B3"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S66PW9R1B3")}</script><meta property="og:url" content="https://mwauracollins.github.io/posts/2025-02-04-gpt/"><meta property="og:site_name" content="mwaura.AI"><meta property="og:title" content="Rolling Your Own GPT"><meta property="og:description" content="Training a GPT model sounds like a moonshot but it is actually just a series of simple and well-defined steps. At its core, it is just a giant text predictor, fed with tons of data and then allowed to guess the next word. The real challenge is not training it to predict the next word, but to make it produce something useful like answers to your assignment due midnight.üòÖ You need some special type of training like Reinforcement Learning with Human Feedback (RLHF). Though you can get good responses without RLHF, RLHF is required to make it chatbot-like."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-04T22:20:47+03:00"><meta property="article:modified_time" content="2025-02-04T22:20:47+03:00"><meta property="article:tag" content="LLMs"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="GPT"><meta name=twitter:card content="summary"><meta name=twitter:title content="Rolling Your Own GPT"><meta name=twitter:description content="Training a GPT model sounds like a moonshot but it is actually just a series of simple and well-defined steps.
At its core, it is just a giant text predictor, fed with tons of data and then allowed to guess the next word. The real challenge is not training it to predict the next word, but to make it produce something useful like answers to your assignment due midnight.üòÖ
You need some special type of training like Reinforcement Learning with Human Feedback (RLHF). Though you can get good responses without RLHF, RLHF is required to make it chatbot-like."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://mwauracollins.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Rolling Your Own GPT","item":"https://mwauracollins.github.io/posts/2025-02-04-gpt/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Rolling Your Own GPT","name":"Rolling Your Own GPT","description":"Training a GPT model sounds like a moonshot but it is actually just a series of simple and well-defined steps. At its core, it is just a giant text predictor, fed with tons of data and then allowed to guess the next word. The real challenge is not training it to predict the next word, but to make it produce something useful like answers to your assignment due midnight.üòÖ You need some special type of training like Reinforcement Learning with Human Feedback (RLHF). Though you can get good responses without RLHF, RLHF is required to make it chatbot-like.\n","keywords":["LLMs","Transformer","GPT"],"articleBody":"Training a GPT model sounds like a moonshot but it is actually just a series of simple and well-defined steps. At its core, it is just a giant text predictor, fed with tons of data and then allowed to guess the next word. The real challenge is not training it to predict the next word, but to make it produce something useful like answers to your assignment due midnight.üòÖ You need some special type of training like Reinforcement Learning with Human Feedback (RLHF). Though you can get good responses without RLHF, RLHF is required to make it chatbot-like.\nWhat This Blog Covers In this blog, we‚Äôll break down the entire pipeline for training a GPT model from scratch. We‚Äôll cover:\nPretraining - Teaching GPT to predict the next word using massive datasets. Finetuning - Adapting the model for specific tasks like chatbots or coding assistants. Alignment (RLHF) - Using human feedback to make GPT useful and safe. By the end, you‚Äôll have a clear roadmap for building your own GPT, whether for research, fun, or world domination. üòà\nIntroduction GPT (Generative Pretrained Transformers) models, as the name suggests, are autoregressive generative models based on the Transformer architecture. The reason it is called generative is because the output of the model is new even with the same input.\nBefore we jump into GPT, let‚Äôs briefly recap on the transformer architecture since GPT is essentially a ‚Äòhalf‚Äô transformer. A full Transformer has both an encoder and a decoder. GPT only uses the decoder.\nA transformer[1] is a machine translation neural network that processes input in parallel and uses self-attention to understand the relationship between words/tokens. Unlike older models like RNNs, which process text sequentially, transformers handle entire sequences at once, making them faster and more efficient.\nIt is composed of two parts:\nEncoder (Left) - Processes the input sequence(used in BERT) Decoder (Right) - Generates the output step-by-step(used in GPT) For more details, please check out Jay Alammar‚Äôs [Blog][jay-allamar] or book for in-depth explanation.\nGPT models specifically use only the decoder part of the transformer and apply causal masking, meaning each token can only attend to past tokens (not future ones). This makes them great for text generation, but without proper fine-tuning, they might just spit out plausible-sounding nonsense.\nPretraining Data preparation After collecting our large text corpus, training data is often filtered and cleaned to remove low-quality content.\nThen, the text is converted to numerical format using tokenization. This can be through Byte-Pair Encoding (BPE), SentencePiece or WordPiece which splits words into smaller subword units (tokens). Simpler models can use character level tokens.\nExample:\n\"I love transformers\" -\u003e [\"I\", \"love\", \"trans\", \"#formers\"] The tokenization process involves a training pipeline that implement encoding - converting a string to a token and decoding - converting a sequence of tokens into a string. Each token is then mapped to a unique ID in a vocabulary which is fed to the model.\nArchitecture Each GPT model is built from multiple stacked decoder blocks.\nToken Embedding - Converts input words/tokens to numerical vectors Positional Encoding - Adds information about word order Decoder Blocks Token Embedding layer Once the text has been tokenized into subword units, it needs to be converted into numerical representations that the model can understand.\nA token embedding is a learned vector representation of a token. Instead of representing tokens as simple IDs (which don‚Äôt capture any meaning), each token is mapped to a high-dimensional vector that contains semantic information.\nFor example, instead of:\n\"I\" \"love\" \"trans\" \"#formers\" -\u003e [102, 304, 753, 9281] The embedding layer converts these token IDs into vectors:\n[102] -\u003e [0.2, -1.3, 0.7, ..., 1.1] [304] -\u003e [-0.4, 2.1, -0.9, ..., 0.3] [753] -\u003e [-0.9, 1.1, 1.2, ..., 0.2] [9281] -\u003e [1.5, -0.7, 0.8, ..., -1.2] Each token is now represented as a dense vector of floating-point numbers, typically in a space of 512 to 4096 dimensions, depending on the model size.\nIt is kind of a look up table with a large matrix $E$ of size (vocab_size x embedding_dim), where each row represents a token and embedding_dim/d_model is the dimension of the vector.\nMathematically, if a token ID is $t_{i}$, the embedding vector is:\n$$ E _{t_{i}} \\in \\mathbb {R} ^ d $$where $d$ is the embedding dimension.\nWhy token embeddings matter:\nCapture Meaning ‚Äì Similar words have similar embeddings (‚Äúdog‚Äù and ‚Äúpuppy‚Äù will be closer in vector space). Reduce Sparsity ‚Äì Unlike one-hot encoding, embeddings are dense representations, making learning more efficient. Enable Transfer Learning ‚Äì Pretrained embeddings help models generalize better with less data. Positional Encoding Since transformers process tokens in parallel, we need positional encoding to retain word order. This is as a result of transformers processing all tokens in parallel and have no built-in understanding of order unlike RNNs.\nInstead of relying on recurrence (like RNNs), transformers add a unique positional vector to each token embedding. This encoding is precomputed and added directly to token embeddings before feeding them into the model.\nThe positional encoding function is defined as:\n$$ \\begin{align*} PE_{(pos+2i)} \u0026= sin \\left( \\frac{pos}{10000^{\\frac{2i}{d}}} \\right) \\\\ PE_{(pos+2i+1)} \u0026= cos \\left( \\frac{pos}{10000^{\\frac{2i}{d}}} \\right) \\end{align*} $$\nHowever, most models use learnable positional embeddings instead of sinusoidal encoding while recent architectures like Llama and Mistral use Rotary Positional Embedding which is covered in recent post.\nThe Decoder Block: The heart of GPT Each decoder block consists of the following key components:\nMasked Multi-Head Self-Attention ‚Äì Ensures each token only attends to past tokens (causal masking). Layer Normalization \u0026 Residual Connection ‚Äì Helps stabilize training. Feedforward Network (FFN) ‚Äì A fully connected layer that refines token representations. Final Layer Normalization \u0026 Residual Connection ‚Äì Another normalization step to maintain stable gradients. A typical GPT model stacks multiple decoder blocks (GPT-3.5 has 96 layers).\nMasked Multi-Head Self-Attention Self-attention allows the model to decide which past tokens are most relevant when generating the next token.\nSelf-Attention Mechanism Self attention is the core mechanism that enables transformers to understand relationships between words/tokens regardless of their position. Unlike RNNs, which process words sequentially, self-attention allows every word to ‚Äúpay attention‚Äù to other words in the input simultaneously.\nThink of it like reading a sentence: when you read a word, your brain doesn‚Äôt just process that word in isolation; you subconsciously connect it to previous words, emphasizing some more than others based on context. This is exactly what self-attention does.\nSelf attention is built around three main concepts:\nQuery (Q) - The word asking, ‚ÄúHey, I am looking for someone who matters? Any Adjective?‚Äù Key (K) - The rest of the words in the sequence replying, ‚ÄúAm I relevant to you?‚Äù Value (V) - The actual information being passed along if the relevance is high. $$ Q = XW_{Q}, \\space \\space K = XW_{K}, \\space \\space V = XW_{V} $$The Q and K also act like a lower dimensional representation of the token‚Äôs embeddings.\nNow, we measure how much focus each word should have on the others. We do this by computing the dot product between the query of a word and the keys of all other words in the sequence. Higher dot product means the vectors have high similarity and a dot product of zero means the vectors are orthogonal and therefore no similariry.\n$$ score = QK^{T} $$This results in an attention score matrix where each row represents how much one token should attend to others.\nI like to think of it like - Being in a crowd and I am the Query and I am trying to figure out which of these people (Keys) yelling ‚ÄúHey, It is me!‚Äù, might be someone I am looking for. The louder and clearer someones‚Äôs voice is to me, the higher the attention score.\nTo prevent large values from making the softmax overly confident, we scale the scores by the square root of the key dimension $d_{k}$\n$$ score = \\frac{QK^{T}}{\\sqrt{d_k}} $$Next, we normalize the attention scores using the softmax function, turning them into probabilities:\n$$ attention\\space weights = softmax(\\frac{QK^{T}}{\\sqrt{d_k}}) $$Each word now has a probability distribution showing how much attention it should give to the other words in the sequence.\nEach token‚Äôs final representation is obtained by multiplying the attention weights with the value vectors.\n$$ attention\\space weights = softmax(\\frac{QK^{T}}{\\sqrt{d_k}}) \\cdot V $$This means each token gets an updated representation that is a weighted sum of all token values, emphasizing the most relevant words.\nMasked Self-Attention GPT is a causal model, it should never ‚Äúsee the future‚Äù when predicting the next word in the sequence. To enforce this, we apply masking of the future tokens when calculating the self-attention.\nThe mask is a triangular matrix with zeros in the upper right half. Any positions corresponding to future words get $-\\inf $ before softmax, making their probabilities effectively zero.\n$$ M = \\begin{bmatrix} A_{11} \u0026 -\\infty \u0026 -\\infty \u0026 -\\infty \\\\ A_{21} \u0026 A_5 \u0026 -\\infty \u0026 -\\infty \\\\ A_{31} \u0026 A_{32} \u0026 A_{33} \u0026 -\\infty \\\\ A_{41} \u0026 A_{42} \u0026 A_{43} \u0026A_{44} \\\\ \\end{bmatrix} $$After softmax:\n$$ M = \\begin{bmatrix} A_{11} \u0026 0 \u0026 0 \u0026 0 \\\\ A_{21} \u0026 A_{22} \u0026 0 \u0026 0 \\\\ A_{31} \u0026 A_{32} \u0026 A_{33} \u0026 0 \\\\ A_{41} \u0026 A_{42} \u0026 A_{43} \u0026 A_{44} \\\\ \\end{bmatrix} $$Multi Head Self-Attention Instead of performing self-attention once, multi-head attention runs multiple attention mechanisms in parallel. Each attention head learns to focus on different aspects of the sentence, capturing various relationships.\nExample(The cat sat on the mat):\nOne head might focus on subject-verb agreement (‚ÄúThe cat sat‚Äù). Another head might track prepositional phrases (‚Äúsat on the mat‚Äù). Each attention head has it own Q, K and V matrices and produce separate attention outputs which are then concatenated and linearly transformed with another matrix.\n$$ MultiHead\\space Attention = Concat(Head_1, Head_2, ..., Head_h) $$Code:\nclass CausalSelfAttention(nn.Module): def __init__(self, d_model: int, n_heads: int, d_k: int, max_seq_length: int): super().__init__() assert d_model % n_heads == 0 self.d_model = d_model self.n_heads = n_heads self.d_k = self.d_model // self.n_heads self.query_proj = nn.Linear(self.d_model, self.n_heads * self.d_k, bias=False) self.key_proj = nn.Linear(self.d_model, self.n_heads * self.d_k, bias=False) self.value_proj = nn.Linear(self.d_model, self.n_heads * self.d_k, bias=False) self.out_proj = nn.Linear(self.n_heads * self.d_k, self.d_model, bias=False) self.register_buffer(\"mask\", torch.tril(torch.ones(max_seq_length, max_seq_length)) * float('-inf')) def forward(self, x): B, T, C = x.shape key = self.key_proj(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2) # B, n_heads, T, d_k query = self.query_proj(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2) value = self.value_proj(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2) attn_weight = (query @ key.transpose(-1, -2)) / math.sqrt(self.d_k) # B, n_heads, T, T attn_weight = attn_weight.masked_fill(self.mask[:T, :T] == 0, float('-inf')) # apply mask attn_weight = F.softmax(attn_weight, dim=-1) # softmax along the last dimension out = (attn_weight @ value).transpose(1, 2).contiguous().view(B, T, C) # B, T, C out = self.out_proj(out) return out Feed Forward Neural Network(FFN) After the self-attention refines the token representations, the next step is to process these representations independently using a simple neural network.\nThe FFN allows the model to extract deeper patterns by introducing non-linearity. This lets the model learn complex features. It also refined the token meaning since the token representations are updates in isolation. This ensures more abstract and useful representation.\nRecent study[2] also point out that the feedforward network is responsible for storing and retrieval of factual knowledge. The self attention helps identify relevant information from the input and the FFN encode the knowledge by acting as key-value stores. The GELU non-linearlity also enhances memorization.\nExample: If the model has learnt that ‚ÄúParis is the capital of France‚Äù, the FFN layers may encode this fact in their weights. When prompted with ‚ÄúThe capital of France is‚Ä¶‚Äù, it matched with the fact. I would think of this like a simple NN with no convolution learning about the MNIST data.\nCode:\nclass FeedForward(nn.Module): def __init__(self, d_model: int): super().__init__() self.linear1 = nn.Linear(d_model, 4 * d_model) self.linear2 = nn.Linear(4 * d_model, d_model) self.gelu = nn.GELU(approximate='tanh') def forward(self, x): return self.linear2(self.gelu(self.linear1(x))) You might wonder, why expand only to shrink it back? Well, this allows feature extractions and information mixing. The non-linearity also highlights key point i.e higher activations for relevant key points.\nGenerating Output At this point, we‚Äôve gone through token embedding, positional encoding, self-attention, and the feedforward network (FFN)‚Äîeach refining the token representations step by step. Now, it‚Äôs time for the final transformation that turns these learned representations into actual words!\nBefore the model produces output, it applies Layer Normalization (LayerNorm) one last time.\n$$ \\text{LN}(x) = \\sigma + \\epsilon \\frac{x - \\mu}{\\sigma} \\cdot \\gamma + \\beta $$At this stage, each token‚Äôs hidden representation has been refined across multiple decoder layers. But the model still operates in vector space, while we need actual word probabilities.\nThe final linear layer acts as a classifier transforming each token representation into a logit vector which is the size of the vocabulary. Each value in the logit vector represent the likelihood of the token to be the next word in the sequence.\nThe model need to pick the next word, right? To do so we convert the logits to probabilities using softmax.\n$$ P(y_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} $$This ensures the output probabilities sum to 1, making it a valid probability distribution.\nThe model selects the token with the highest probability (or samples based on a strategy like temperature or top-k sampling).\nOnce we have probabilities, the model picks the next token and repeats the process iteratively until:\nA stop token is reached. A max length is hit. The output meets some quality threshold. Each new token is fed back into the model, and the process continues autoregressively, token by token until we get a full response.\nYou can check out the code for pretraining on my github\nFinetuning So far, we have trained a GPT model as a next token predictor. But raw GPT models (Base Models) can still generate nonsense, bias and irrelevant text. To refine them into useful AI assistants, we apply supervised fine-tuning(SFT)\nSupervised Fine-Tuning SFT is the process of training the model on high-quality, task-specific data with labeled examples. Instead of predicting the next token from raw internet text, the model learns from human-curated inputs and outputs to become more aligned with what we want.\nFor this, we need high quality labelled data on the input and output pairs showing the desired behaviour. This data must be clean, diverse and instruction like.\nExample of a dataset:\nDuring fine-tuning, the model‚Äôs weights are updated using cross-entropy loss between its predictions and the correct answers from the dataset.\nStep 1: Model sees an input (e.g. ‚ÄúWhat is the capital of Japan?‚Äù)\nStep 2: It generates an output (e.g. ‚ÄúOsaka‚Äù ‚ùå)\nStep 3: The loss function compares this output to the correct one (‚ÄúTokyo‚Äù ‚úÖ)\nStep 4: Backpropagation updates weights to reduce errors.\nThe fine-tuning objective is still next-token prediction, but now on a curated dataset, ensuring responses become more helpful, accurate, and structured. It is also trained with special tokens \u003c|user|\u003e which marks the beginning of the user‚Äôs input and \u003c|assistant|\u003e which marks the beginning of the assistant‚Äôs response.\nSFT usually takes days to weeks depending on model size and data quantity, but it is far cheaper than training from scratch.\nWhy SFT is important?\nIt improves factual accuracy which in turn reduces hallucinations. Makes it user-friendly by ensuring polite, safe, and clear responses. Prepares for RLHF Without SFT, a base GPT model is like a highly intelligent but untrained intern üòÖ\nRLHF Even after Supervised Fine-Tuning (SFT), the model still has flaws as it can generate biased, misleading, or undesired responses. To further refine its behavior, we apply RLHF, a reinforcement learning approach where human preferences guide the model‚Äôs training.\nThis is because SFT helps the model learn what to say, but RLHF helps it learn how to say it in a way that aligns with human values. Without it, the fine-tuned model may still provide technically correct but unhelpful responses or too verbose or too brief responses.\nThis is particular in mathematical tasks where we need the model to respond with the steps first then the answer.\nSteps in RLHF Supervised finetuning Reward modelling - Training a separate reward model that scores different responses based on human preference Reinforcement Learning with Proximal Policy Optimization (PPO) Training the reward model A reward model acts like a quality evaluator for the response.\nTo train it, we need a dataset where human annotators rank multiple completions of a prompt.\nAn example dataset: The reward model is then trained to assign higher scores to preferred responses. It learns from human preference data which helps the AI avoid unsafe or misleading outputs. Let‚Äôs define a reward function R that assigns a score R(x, y) to a response y given a prompt x.\nTo train this model, we minimize the loss:\n$$ L = -\\log_\\sigma (R(x, y_+) - R(x, y_-)) $$where:\n$y_+$ is the preferred response $y_-$ is the less-preferred response $\\sigma$ is the sigmoid function ($\\frac{1}{1 + e^{-z}}$) which ensures the score stay within the valid range. $R(x, y_+)$ is the reward assigned to the preferred choice $R(x, y_-)$ is the reward assigned to the less-preferred choice $$ \\text{If } R(x,y_+) \\gg R(x,y_-), \\text{ then } \\sigma(R(x,y_+) - R(x,y_-)) \\text{ is close to 1, and } -\\log(1) = 0, \\text{ meaning low loss (good).} $$$$ \\text{If } R(x,y_+) \\leq R(x,y_-), \\text{ then } \\sigma(R(x,y_+) - R(x,y_-)) \\text{ is small, and } -\\log(\\text{small number}) \\text{ is high loss (bad).} $$In short, the loss penalizes cases where the preferred option doesn‚Äôt get a higher reward than the non-preferred one.\nFine-tuning with Reinforcement Learning We have a reward model but where does it come in. Well, we use Proximal Policy Optimization (PPO) to train GPT to generate responses that maximize reward scores.\nGPT generates a response $y$ for a given input $x$, the reward model $R(x, y)$ assigns a score whether it is a good response of not and finally the PPO adjusts the GPT‚Äôs weights to favour responses that get higher scores.\nThe policy update is:\n$$ \\theta' = \\theta + \\alpha \\nabla_\\theta E[R(x,y)] $$where:\n$\\theta$ are the GPT model weights $\\alpha$ is the learning rate $R(x,y)$ is the reward function. PPO prevents over-updating by limiting how much the policy can change per step.\nReferences Attention Is All You Need ‚Äì The original Transformer paper by Vaswani et al. (2017). Jay Alammar‚Äôs Blog ‚Äì An excellent visual explanation of Transformers. Jay Alammar‚Äôs Book: Hands-On Large Language Models ‚Äì A practical guide to understanding and implementing LLMs. ‚ÄúTransformer Feed-Forward Layers Are Key-Value Memories‚Äù (Geva et al., 2021) Reinforcement Learning with Human Feedback (RLHF) (Christiano et al., 2017) Language Models Are Few-Shot Learners (Brown et al., 2020) ‚Äì The GPT-3 paper demonstrating few-shot, zero-shot, and one-shot learning capabilities. Multitask Prompted Training Enables Zero-Shot Task Generalization (Sanh et al., 2022) ‚Äì A study on how large language models can generalize across tasks. Fine-Tuning Strategies for Large Language Models (Howard \u0026 Ruder, 2018) ‚Äì A paper discussing supervised fine-tuning (SFT) approaches. Byte Pair Encoding (BPE) SentencePiece Tokenizer My GitHub Repository ‚Äì My personal GPT-2 implementation ","wordCount":"3146","inLanguage":"en","datePublished":"2025-02-04T22:20:47+03:00","dateModified":"2025-02-04T22:20:47+03:00","author":{"@type":"Person","name":"Mwaura Collins"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mwauracollins.github.io/posts/2025-02-04-gpt/"},"publisher":{"@type":"Organization","name":"mwaura.AI","logo":{"@type":"ImageObject","url":"https://mwauracollins.github.io/favicon.ico"}}}</script><script>MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]]}}</script><script defer src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mwauracollins.github.io/ accesskey=h title="mwaura.AI (Alt + H)">mwaura.AI</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mwauracollins.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://mwauracollins.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://mwaura.tech/ title=mwaura.tech><span>mwaura.tech</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mwauracollins.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://mwauracollins.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Rolling Your Own GPT</h1><div class=post-meta><span title='2025-02-04 22:20:47 +0300 EAT'>February 4, 2025</span>&nbsp;¬∑&nbsp;15 min&nbsp;¬∑&nbsp;3146 words&nbsp;¬∑&nbsp;Mwaura Collins</div></header><div class=post-content><p>Training a GPT model sounds like a moonshot but it is actually just a series of simple and well-defined steps.
At its core, it is just a giant text predictor, fed with tons of data and then allowed to guess the next word. The real challenge is not training it to predict the next word, but to make it produce something useful like answers to your assignment due midnight.üòÖ
You need some special type of training like <strong>Reinforcement Learning with Human Feedback <a href=#rlhf>(RLHF)</a></strong>. Though you can get good responses without RLHF, RLHF is required to make it chatbot-like.</p><h2 id=what-this-blog-covers>What This Blog Covers<a hidden class=anchor aria-hidden=true href=#what-this-blog-covers>#</a></h2><p>In this blog, we&rsquo;ll break down the entire pipeline for training a GPT model from scratch. We&rsquo;ll cover:</p><ol><li><a href=#pretraining>Pretraining</a> - Teaching GPT to predict the next word using massive datasets.</li><li><a href=#finetuning>Finetuning</a> - Adapting the model for specific tasks like chatbots or coding assistants.</li><li><a href=#rlhf>Alignment (RLHF)</a> - Using human feedback to make GPT useful and safe.</li></ol><p>By the end, you&rsquo;ll have a clear roadmap for building your own GPT, whether for research, fun, or world domination. üòà</p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>GPT (Generative Pretrained Transformers) models, as the name suggests, are
<strong>autoregressive generative models</strong> based on the Transformer architecture. The reason it is called generative is because the output of the model is new even with the same input.</p><p>Before we jump into GPT, let&rsquo;s briefly recap on the transformer architecture since GPT is essentially a &lsquo;half&rsquo; transformer. A full Transformer has both an encoder and a decoder. GPT only uses the decoder.</p><p>A transformer<a href=https://arxiv.org/abs/1706.03762>[1]</a> is a machine translation neural network that processes input in parallel and uses <a href=#self-attention>self-attention</a> to understand the relationship between words/tokens. Unlike older models like RNNs, which process text sequentially, transformers handle entire sequences at once, making them faster and more efficient.</p><div align=center><img src=/images/gpt/transformer.png alt=Transformer width=300 height=400></div><p>It is composed of two parts:</p><ul><li>Encoder (Left) - Processes the input sequence(used in BERT)</li><li>Decoder (Right) - Generates the output step-by-step(used in GPT)</li></ul><p>For more details, please check out Jay Alammar&rsquo;s [Blog][jay-allamar] or <a href=https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961>book</a> for in-depth explanation.</p><p>GPT models specifically use only the decoder part of the transformer and apply causal masking, meaning each token can only attend to past tokens (not future ones). This makes them great for text generation, but without proper fine-tuning, they might just spit out plausible-sounding nonsense.</p><h2 id=pretraining>Pretraining<a hidden class=anchor aria-hidden=true href=#pretraining>#</a></h2><h3 id=data-preparation>Data preparation<a hidden class=anchor aria-hidden=true href=#data-preparation>#</a></h3><p>After collecting our large text corpus, training data is often filtered and cleaned to remove low-quality content.</p><p>Then, the text is converted to numerical format using tokenization. This can be through <a href="https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt"><strong>Byte-Pair Encoding (BPE)</strong></a>, <a href=https://github.com/google/sentencepiece><strong>SentencePiece</strong></a> or <a href=https://github.com/google/sentencepiece><strong>WordPiece</strong></a> which splits words into smaller subword units (tokens). Simpler models can use character level tokens.</p><p>Example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&#34;I love transformers&#34; -&gt; [&#34;I&#34;, &#34;love&#34;, &#34;trans&#34;, &#34;#formers&#34;]
</span></span></code></pre></div><p>The tokenization process involves a training pipeline that implement encoding - converting a string to a token and decoding - converting a sequence of tokens into a string.
Each token is then mapped to a unique ID in a vocabulary which is fed to the model.</p><h3 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h3><p>Each GPT model is built from multiple stacked decoder blocks.</p><p><img loading=lazy src=/images/gpt/decoder.png></p><ol><li>Token Embedding - Converts input words/tokens to numerical vectors</li><li>Positional Encoding - Adds information about word order</li><li>Decoder Blocks</li></ol><h4 id=token-embedding-layer>Token Embedding layer<a hidden class=anchor aria-hidden=true href=#token-embedding-layer>#</a></h4><p>Once the text has been tokenized into subword units, it needs to be converted into numerical representations that the model can understand.</p><p>A token embedding is a learned vector representation of a token. Instead of representing tokens as simple IDs (which don‚Äôt capture any meaning), each token is mapped to a high-dimensional vector that contains semantic information.</p><p>For example, instead of:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&#34;I&#34; &#34;love&#34; &#34;trans&#34; &#34;#formers&#34; -&gt; [102, 304, 753, 9281]
</span></span></code></pre></div><p>The embedding layer converts these token IDs into vectors:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>[102] -&gt; [0.2, -1.3, 0.7, ..., 1.1]
</span></span><span style=display:flex><span>[304] -&gt; [-0.4, 2.1, -0.9, ..., 0.3]
</span></span><span style=display:flex><span>[753] -&gt; [-0.9, 1.1, 1.2, ..., 0.2]
</span></span><span style=display:flex><span>[9281] -&gt; [1.5, -0.7, 0.8, ..., -1.2]
</span></span></code></pre></div><p>Each token is now represented as a dense vector of floating-point numbers, typically in a space of 512 to 4096 dimensions, depending on the model size.</p><p>It is kind of a look up table with a large matrix $E$ of size <em>(vocab_size x embedding_dim)</em>, where each row represents a token and <em>embedding_dim/d_model</em> is the dimension of the vector.</p><p>Mathematically, if a token ID is $t_{i}$, the embedding vector is:</p>$$
E _{t_{i}} \in \mathbb {R} ^ d
$$<p>where $d$ is the embedding dimension.</p><p>Why token embeddings matter:</p><ul><li>Capture Meaning ‚Äì Similar words have similar embeddings (&ldquo;dog&rdquo; and &ldquo;puppy&rdquo; will be closer in vector space).</li><li>Reduce Sparsity ‚Äì Unlike one-hot encoding, embeddings are dense representations, making learning more efficient.</li><li>Enable Transfer Learning ‚Äì Pretrained embeddings help models generalize better with less data.</li></ul><h4 id=positional-encoding>Positional Encoding<a hidden class=anchor aria-hidden=true href=#positional-encoding>#</a></h4><p>Since transformers process tokens in parallel, we need positional encoding to retain word order. This is as a result of transformers processing all tokens in parallel and have no built-in understanding of order unlike RNNs.</p><p>Instead of relying on recurrence (like RNNs), transformers add a unique positional vector to each token embedding. This encoding is precomputed and added directly to token embeddings before feeding them into the model.</p><p>The positional encoding function is defined as:</p>$$
\begin{align*}
PE_{(pos+2i)} &= sin \left( \frac{pos}{10000^{\frac{2i}{d}}} \right) \\
PE_{(pos+2i+1)} &= cos \left( \frac{pos}{10000^{\frac{2i}{d}}} \right)
\end{align*}
$$<p><img loading=lazy src=https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1f37014-74da-4a51-991f-d7212e527df3_1314x784.png></p><p>However, most models use learnable positional embeddings instead of sinusoidal encoding while recent architectures like Llama and Mistral use Rotary Positional Embedding which is covered in recent <a href=/posts/2025-01-24-rope>post</a>.</p><h4 id=the-decoder-block-the-heart-of-gpt>The Decoder Block: The heart of GPT<a hidden class=anchor aria-hidden=true href=#the-decoder-block-the-heart-of-gpt>#</a></h4><p>Each decoder block consists of the following key components:</p><ol><li>Masked Multi-Head Self-Attention ‚Äì Ensures each token only attends to past tokens (causal masking).</li><li>Layer Normalization & Residual Connection ‚Äì Helps stabilize training.</li><li>Feedforward Network (FFN) ‚Äì A fully connected layer that refines token representations.</li><li>Final Layer Normalization & Residual Connection ‚Äì Another normalization step to maintain stable gradients.</li></ol><p>A typical GPT model stacks multiple decoder blocks (GPT-3.5 has 96 layers).</p><div align=center><img src=/images/gpt/decoder-block.png alt=Transformer width=300 height=400></div><h2 id=masked-multi-head-self-attention>Masked Multi-Head Self-Attention<a hidden class=anchor aria-hidden=true href=#masked-multi-head-self-attention>#</a></h2><p>Self-attention allows the model to decide which past tokens are most relevant when generating the next token.</p><h3 id=self-attention-mechanism>Self-Attention Mechanism<a hidden class=anchor aria-hidden=true href=#self-attention-mechanism>#</a></h3><p>Self attention is the core mechanism that enables transformers to understand relationships between words/tokens regardless of their position. Unlike RNNs, which process words sequentially, self-attention allows every word to &ldquo;pay attention&rdquo; to other words in the input simultaneously.</p><p>Think of it like reading a sentence: when you read a word, your brain doesn&rsquo;t just process that word in isolation; you subconsciously connect it to previous words, emphasizing some more than others based on context. This is exactly what self-attention does.</p><p>Self attention is built around three main concepts:</p><ul><li>Query (Q) - The word asking, &ldquo;Hey, I am looking for someone who matters? Any Adjective?&rdquo;</li><li>Key (K) - The rest of the words in the sequence replying, &ldquo;Am I relevant to you?&rdquo;</li><li>Value (V) - The actual information being passed along if the relevance is high.</li></ul>$$
Q = XW_{Q}, \space \space K = XW_{K}, \space \space V = XW_{V}
$$<p>The Q and K also act like a lower dimensional representation of the token&rsquo;s embeddings.</p><p>Now, we measure how much focus each word should have on the others. We do this by computing the dot product between the query of a word and the keys of all other words in the sequence. Higher dot product means the vectors have high similarity and a dot product of zero means the vectors are orthogonal and therefore no similariry.</p>$$
score = QK^{T}
$$<p>This results in an attention score matrix where each row represents how much one token should attend to others.</p><hr><p>I like to think of it like - Being in a crowd and I am the Query and I am trying to figure out which of these people (Keys) yelling &ldquo;Hey, It is me!&rdquo;, might be someone I am looking for. The louder and clearer someones&rsquo;s voice is to me, the higher the attention score.</p><hr><p>To prevent large values from making the softmax overly confident, we scale the scores by the square root of the key dimension $d_{k}$</p>$$
score = \frac{QK^{T}}{\sqrt{d_k}}
$$<p>Next, we normalize the attention scores using the softmax function, turning them into probabilities:</p>$$
attention\space weights = softmax(\frac{QK^{T}}{\sqrt{d_k}})
$$<p>Each word now has a probability distribution showing how much attention it should give to the other words in the sequence.</p><p>Each token‚Äôs final representation is obtained by multiplying the attention weights with the value vectors.</p>$$
attention\space weights = softmax(\frac{QK^{T}}{\sqrt{d_k}}) \cdot V
$$<p>This means each token gets an updated representation that is a weighted sum of all token values, emphasizing the most relevant words.</p><h3 id=masked-self-attention>Masked Self-Attention<a hidden class=anchor aria-hidden=true href=#masked-self-attention>#</a></h3><p>GPT is a causal model, it should never &ldquo;see the future&rdquo; when predicting the next word in the sequence. To enforce this, we apply masking of the future tokens when calculating the self-attention.</p><p>The mask is a triangular matrix with zeros in the upper right half. Any positions corresponding to future words get $-\inf $ before softmax, making their probabilities effectively zero.</p>$$
M = \begin{bmatrix}
A_{11} & -\infty & -\infty & -\infty \\
A_{21} & A_5 & -\infty & -\infty \\
A_{31} & A_{32} & A_{33} & -\infty \\
A_{41} & A_{42} & A_{43} &A_{44} \\
\end{bmatrix}
$$<p>After softmax:</p>$$
M = \begin{bmatrix}
A_{11} & 0 & 0 & 0 \\
A_{21} & A_{22} & 0 & 0 \\
A_{31} & A_{32} & A_{33} & 0 \\
A_{41} & A_{42} & A_{43} & A_{44} \\
\end{bmatrix}
$$<h3 id=multi-head-self-attention>Multi Head Self-Attention<a hidden class=anchor aria-hidden=true href=#multi-head-self-attention>#</a></h3><p>Instead of performing self-attention once, multi-head attention runs multiple attention mechanisms in parallel. Each attention head learns to focus on different aspects of the sentence, capturing various relationships.</p><p>Example(The cat sat on the mat):</p><ul><li>One head might focus on subject-verb agreement (&ldquo;The cat sat&rdquo;).</li><li>Another head might track prepositional phrases (&ldquo;sat on the mat&rdquo;).</li></ul><p>Each attention head has it own Q, K and V matrices and produce separate attention outputs which are then concatenated and linearly transformed with another matrix.</p>$$
MultiHead\space Attention = Concat(Head_1, Head_2, ..., Head_h)
$$<p>Code:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CausalSelfAttention</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, d_model: int, n_heads: int, d_k: int, max_seq_length: int):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> d_model <span style=color:#f92672>%</span> n_heads <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>d_model <span style=color:#f92672>=</span> d_model
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_heads <span style=color:#f92672>=</span> n_heads
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>d_k <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>d_model <span style=color:#f92672>//</span> self<span style=color:#f92672>.</span>n_heads
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>query_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>d_model, self<span style=color:#f92672>.</span>n_heads <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>d_k, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>key_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>d_model, self<span style=color:#f92672>.</span>n_heads <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>d_k, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>value_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>d_model, self<span style=color:#f92672>.</span>n_heads <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>d_k, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>out_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>n_heads <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>d_k, self<span style=color:#f92672>.</span>d_model, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>register_buffer(<span style=color:#e6db74>&#34;mask&#34;</span>, torch<span style=color:#f92672>.</span>tril(torch<span style=color:#f92672>.</span>ones(max_seq_length, max_seq_length)) <span style=color:#f92672>*</span> float(<span style=color:#e6db74>&#39;-inf&#39;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        B, T, C <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        key <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>key_proj(x)<span style=color:#f92672>.</span>view(B, T, self<span style=color:#f92672>.</span>n_heads, self<span style=color:#f92672>.</span>d_k)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>) <span style=color:#75715e># B, n_heads, T, d_k</span>
</span></span><span style=display:flex><span>        query <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>query_proj(x)<span style=color:#f92672>.</span>view(B, T, self<span style=color:#f92672>.</span>n_heads, self<span style=color:#f92672>.</span>d_k)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        value <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>value_proj(x)<span style=color:#f92672>.</span>view(B, T, self<span style=color:#f92672>.</span>n_heads, self<span style=color:#f92672>.</span>d_k)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        attn_weight <span style=color:#f92672>=</span> (query <span style=color:#f92672>@</span> key<span style=color:#f92672>.</span>transpose(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>)) <span style=color:#f92672>/</span> math<span style=color:#f92672>.</span>sqrt(self<span style=color:#f92672>.</span>d_k)  <span style=color:#75715e># B, n_heads, T, T</span>
</span></span><span style=display:flex><span>        attn_weight <span style=color:#f92672>=</span> attn_weight<span style=color:#f92672>.</span>masked_fill(self<span style=color:#f92672>.</span>mask[:T, :T] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>, float(<span style=color:#e6db74>&#39;-inf&#39;</span>))  <span style=color:#75715e># apply mask</span>
</span></span><span style=display:flex><span>        attn_weight <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(attn_weight, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)  <span style=color:#75715e># softmax along the last dimension</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> (attn_weight <span style=color:#f92672>@</span> value)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>contiguous()<span style=color:#f92672>.</span>view(B, T, C)  <span style=color:#75715e># B, T, C</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>out_proj(out)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> out
</span></span></code></pre></div><h4 id=feed-forward-neural-networkffn>Feed Forward Neural Network(FFN)<a hidden class=anchor aria-hidden=true href=#feed-forward-neural-networkffn>#</a></h4><p>After the self-attention refines the token representations, the next step is to process these representations independently using a simple neural network.</p><p>The FFN allows the model to extract deeper patterns by introducing non-linearity. This lets the model learn complex features. It also refined the token meaning since the token representations are updates in isolation. This ensures more abstract and useful representation.</p><p>Recent study<a href=https://arxiv.org/abs/2106.02739>[2]</a> also point out that the feedforward network is responsible for storing and retrieval of factual knowledge. The self attention helps identify relevant information from the input and the FFN encode the knowledge by acting as key-value stores. The GELU non-linearlity also enhances memorization.</p><p>Example:
If the model has learnt that &ldquo;Paris is the capital of France&rdquo;, the FFN layers may encode this fact in their weights. When prompted with &ldquo;The capital of France is&mldr;&rdquo;, it matched with the fact. I would think of this like a simple NN with no convolution learning about the MNIST data.</p><p>Code:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>FeedForward</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, d_model: int):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_model, <span style=color:#ae81ff>4</span> <span style=color:#f92672>*</span> d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>4</span> <span style=color:#f92672>*</span> d_model, d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gelu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>GELU(approximate<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tanh&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>linear2(self<span style=color:#f92672>.</span>gelu(self<span style=color:#f92672>.</span>linear1(x)))
</span></span></code></pre></div><p>You might wonder, why expand only to shrink it back? Well, this allows feature extractions and information mixing. The non-linearity also highlights key point i.e higher activations for relevant key points.</p><h3 id=generating-output>Generating Output<a hidden class=anchor aria-hidden=true href=#generating-output>#</a></h3><p>At this point, we‚Äôve gone through token embedding, positional encoding, self-attention, and the feedforward network (FFN)‚Äîeach refining the token representations step by step. Now, it&rsquo;s time for the final transformation that turns these learned representations into actual words!</p><p>Before the model produces output, it applies Layer Normalization (LayerNorm) one last time.</p>$$
\text{LN}(x) = \sigma + \epsilon \frac{x - \mu}{\sigma} \cdot \gamma + \beta
$$<p>At this stage, each token‚Äôs hidden representation has been refined across multiple decoder layers. But the model still operates in vector space, while we need actual word probabilities.</p><p>The final linear layer acts as a classifier transforming each token representation into a <strong>logit</strong> vector which is the size of the vocabulary. Each value in the logit vector represent the likelihood of the token to be the next word in the sequence.</p><p>The model need to pick the next word, right? To do so we convert the logits to probabilities using softmax.</p>$$
P(y_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$<p>This ensures the output probabilities sum to 1, making it a valid probability distribution.</p><p>The model selects the token with the highest probability (or samples based on a strategy like temperature or top-k sampling).</p><p>Once we have probabilities, the model picks the next token and repeats the process iteratively until:</p><ul><li>A stop token is reached.</li><li>A max length is hit.</li><li>The output meets some quality threshold.</li></ul><p>Each new token is fed back into the model, and the process continues autoregressively, token by token until we get a full response.</p><p>You can check out the code for pretraining on my <a href=https://github.com/Mwauracollins/gpt2>github</a></p><h2 id=finetuning>Finetuning<a hidden class=anchor aria-hidden=true href=#finetuning>#</a></h2><p>So far, we have trained a GPT model as a next token predictor. But raw GPT models (Base Models) can still generate nonsense, bias and irrelevant text. To refine them into useful AI assistants, we apply <strong>supervised fine-tuning(SFT)</strong></p><h3 id=supervised-fine-tuning>Supervised Fine-Tuning<a hidden class=anchor aria-hidden=true href=#supervised-fine-tuning>#</a></h3><p>SFT is the process of training the model on high-quality, task-specific data with labeled examples. Instead of predicting the next token from raw internet text, the model learns from human-curated inputs and outputs to become more aligned with what we want.</p><p>For this, we need high quality labelled data on the input and output pairs showing the desired behaviour. This data must be clean, diverse and instruction like.</p><p>Example of a dataset:</p><div align=center><img src=/images/gpt/image.png alt=Transformer></div><p>During fine-tuning, the model‚Äôs weights are updated using cross-entropy loss between its predictions and the correct answers from the dataset.</p><p>Step 1: Model sees an input (e.g. &ldquo;What is the capital of Japan?&rdquo;)</p><p>Step 2: It generates an output (e.g. &ldquo;Osaka&rdquo; ‚ùå)</p><p>Step 3: The loss function compares this output to the correct one (&ldquo;Tokyo&rdquo; ‚úÖ)</p><p>Step 4: Backpropagation updates weights to reduce errors.</p><p>The fine-tuning objective is still next-token prediction, but now on a curated dataset, ensuring responses become more helpful, accurate, and structured. It is also trained with special tokens <code>&lt;|user|></code> which marks the beginning of the user&rsquo;s input and <code>&lt;|assistant|></code> which marks the beginning of the assistant&rsquo;s response.</p><p>SFT usually takes days to weeks depending on model size and data quantity, but it is far cheaper than training from scratch.</p><p>Why SFT is important?</p><ul><li>It improves factual accuracy which in turn reduces hallucinations.</li><li>Makes it user-friendly by ensuring polite, safe, and clear responses.</li><li>Prepares for RLHF</li></ul><p>Without SFT, a base GPT model is like a highly intelligent but untrained intern üòÖ</p><h2 id=rlhf>RLHF<a hidden class=anchor aria-hidden=true href=#rlhf>#</a></h2><p>Even after Supervised Fine-Tuning (SFT), the model still has flaws as it can generate biased, misleading, or undesired responses. To further refine its behavior, we apply RLHF, a reinforcement learning approach where human preferences guide the model‚Äôs training.</p><p>This is because SFT helps the model learn what to say, but RLHF helps it learn how to say it in a way that aligns with human values. Without it, the fine-tuned model may still provide technically correct but unhelpful responses or too verbose or too brief responses.</p><p>This is particular in mathematical tasks where we need the model to respond with the steps first then the answer.</p><h3 id=steps-in-rlhf>Steps in RLHF<a hidden class=anchor aria-hidden=true href=#steps-in-rlhf>#</a></h3><ol><li>Supervised finetuning</li><li>Reward modelling - Training a separate reward model that scores different responses based on human preference</li><li>Reinforcement Learning with Proximal Policy Optimization (PPO)</li></ol><h4 id=training-the-reward-model>Training the reward model<a hidden class=anchor aria-hidden=true href=#training-the-reward-model>#</a></h4><p>A reward model acts like a quality evaluator for the response.</p><p>To train it, we need a dataset where human annotators rank multiple completions of a prompt.</p><p>An example dataset:
<img loading=lazy src=https://miro.medium.com/v2/resize:fit:720/format:webp/1*lNi5BN8DAzTr6ztDFrsZ9g.png></p><p>The reward model is then trained to assign higher scores to preferred responses. It learns from human preference data which helps the AI avoid unsafe or misleading outputs.
Let‚Äôs define a reward function R that assigns a score R(x, y) to a response y given a prompt x.</p><p>To train this model, we minimize the loss:</p>$$
L = -\log_\sigma (R(x, y_+) - R(x, y_-))
$$<p>where:</p><ul><li>$y_+$ is the preferred response</li><li>$y_-$ is the less-preferred response</li><li>$\sigma$ is the sigmoid function ($\frac{1}{1 + e^{-z}}$) which ensures the score stay within the valid range.</li><li>$R(x, y_+)$ is the reward assigned to the preferred choice</li><li>$R(x, y_-)$ is the reward assigned to the less-preferred choice</li></ul>$$
\text{If } R(x,y_+) \gg R(x,y_-), \text{ then } \sigma(R(x,y_+) - R(x,y_-)) \text{ is close to 1, and } -\log(1) = 0, \text{ meaning low loss (good).}
$$$$
\text{If } R(x,y_+) \leq R(x,y_-), \text{ then } \sigma(R(x,y_+) - R(x,y_-)) \text{ is small, and } -\log(\text{small number}) \text{ is high loss (bad).}
$$<p>In short, the loss penalizes cases where the preferred option doesn‚Äôt get a higher reward than the non-preferred one.</p><h4 id=fine-tuning-with-reinforcement-learning>Fine-tuning with Reinforcement Learning<a hidden class=anchor aria-hidden=true href=#fine-tuning-with-reinforcement-learning>#</a></h4><p>We have a reward model but where does it come in. Well, we use Proximal Policy Optimization (PPO) to train GPT to generate responses that maximize reward scores.</p><p>GPT generates a response $y$ for a given input $x$, the reward model $R(x, y)$ assigns a score whether it is a good response of not and finally the PPO adjusts the GPT&rsquo;s weights to favour responses that get higher scores.</p><p>The policy update is:</p>$$
\theta' = \theta + \alpha \nabla_\theta E[R(x,y)]
$$<p>where:</p><ul><li>$\theta$ are the GPT model weights</li><li>$\alpha$ is the learning rate</li><li>$R(x,y)$ is the reward function.</li></ul><p>PPO prevents over-updating by limiting how much the policy can change per step.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><strong><a href=https://arxiv.org/abs/1706.03762>Attention Is All You Need</a></strong> ‚Äì The original Transformer paper by Vaswani et al. (2017).</li><li><strong><a href=https://jalammar.github.io/illustrated-transformer/>Jay Alammar&rsquo;s Blog</a></strong> ‚Äì An excellent visual explanation of Transformers.</li><li><strong><a href=https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961>Jay Alammar&rsquo;s Book: Hands-On Large Language Models</a></strong> ‚Äì A practical guide to understanding and implementing LLMs.</li><li><strong><a href=https://arxiv.org/abs/2106.02739>&ldquo;Transformer Feed-Forward Layers Are Key-Value Memories&rdquo;</a></strong> (Geva et al., 2021)</li><li><strong><a href=https://arxiv.org/abs/1706.03741>Reinforcement Learning with Human Feedback (RLHF)</a></strong> (Christiano et al., 2017)</li><li><strong><a href=https://arxiv.org/abs/2005.14165>Language Models Are Few-Shot Learners</a></strong> (Brown et al., 2020) ‚Äì The GPT-3 paper demonstrating few-shot, zero-shot, and one-shot learning capabilities.</li><li><strong><a href=https://arxiv.org/abs/2110.08207>Multitask Prompted Training Enables Zero-Shot Task Generalization</a></strong> (Sanh et al., 2022) ‚Äì A study on how large language models can generalize across tasks.</li><li><strong><a href=https://arxiv.org/abs/1801.06146>Fine-Tuning Strategies for Large Language Models</a></strong> (Howard & Ruder, 2018) ‚Äì A paper discussing supervised fine-tuning (SFT) approaches.</li><li><strong><a href="https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt">Byte Pair Encoding (BPE)</a></strong></li><li><strong><a href=https://github.com/google/sentencepiece>SentencePiece Tokenizer</a></strong></li><li><strong><a href=https://github.com/Mwauracollins/gpt2>My GitHub Repository</a></strong> ‚Äì My personal GPT-2 implementation</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://mwauracollins.github.io/tags/llms/>LLMs</a></li><li><a href=https://mwauracollins.github.io/tags/transformer/>Transformer</a></li><li><a href=https://mwauracollins.github.io/tags/gpt/>GPT</a></li></ul><nav class=paginav><a class=prev href=https://mwauracollins.github.io/posts/2025-02-23-rl_bandits/><span class=title>¬´ Prev</span><br><span>Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making</span>
</a><a class=next href=https://mwauracollins.github.io/posts/2025-01-24-rope/><span class=title>Next ¬ª</span><br><span>Unraveling RoPE: Encoding Relative Positions in Transformers with Elegance</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Rolling Your Own GPT on x" href="https://x.com/intent/tweet/?text=Rolling%20Your%20Own%20GPT&amp;url=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-04-gpt%2f&amp;hashtags=LLMs%2cTransformer%2cGPT"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Rolling Your Own GPT on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-04-gpt%2f&amp;title=Rolling%20Your%20Own%20GPT&amp;summary=Rolling%20Your%20Own%20GPT&amp;source=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-04-gpt%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Rolling Your Own GPT on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-04-gpt%2f&title=Rolling%20Your%20Own%20GPT"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Rolling Your Own GPT on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-04-gpt%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Rolling Your Own GPT on whatsapp" href="https://api.whatsapp.com/send?text=Rolling%20Your%20Own%20GPT%20-%20https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-04-gpt%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Rolling Your Own GPT on telegram" href="https://telegram.me/share/url?text=Rolling%20Your%20Own%20GPT&amp;url=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-04-gpt%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Rolling Your Own GPT on ycombinator" href="https://news.ycombinator.com/submitlink?t=Rolling%20Your%20Own%20GPT&u=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-04-gpt%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://mwauracollins.github.io/>mwaura.AI</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>