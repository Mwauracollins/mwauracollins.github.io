<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making | mwaura.AI</title>
<meta name=keywords content="Reinforcement Learning,Bandits,Mathy"><meta name=description content="Every day, we interact with our world and make decisions based on experience. Whether eating out or whether to use the stairs or the elevator, every day we make a decision. Sometimes we&rsquo;re doing what we know and sometimes we&rsquo;re winging it with something new. These are a form of reinforcement learning. This form of learning is at the heart of most living things; with infants learning to walk by making mistakes and elephants in a zoo learning that electric fence must be kept away from."><meta name=author content="Mwaura Collins"><link rel=canonical href=https://mwauracollins.github.io/posts/2025-02-23-rl_bandits/><meta name=google-site-verification content="G-S66PW9R1B3"><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css rel=stylesheet><link rel=icon href=https://mwauracollins.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mwauracollins.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mwauracollins.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mwauracollins.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mwauracollins.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mwauracollins.github.io/posts/2025-02-23-rl_bandits/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-S66PW9R1B3"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S66PW9R1B3")}</script><meta property="og:url" content="https://mwauracollins.github.io/posts/2025-02-23-rl_bandits/"><meta property="og:site_name" content="mwaura.AI"><meta property="og:title" content="Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making"><meta property="og:description" content="Every day, we interact with our world and make decisions based on experience. Whether eating out or whether to use the stairs or the elevator, every day we make a decision. Sometimes we‚Äôre doing what we know and sometimes we‚Äôre winging it with something new. These are a form of reinforcement learning. This form of learning is at the heart of most living things; with infants learning to walk by making mistakes and elephants in a zoo learning that electric fence must be kept away from."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-23T22:30:36+03:00"><meta property="article:modified_time" content="2025-02-23T22:30:36+03:00"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Bandits"><meta property="article:tag" content="Mathy"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making"><meta name=twitter:description content="Every day, we interact with our world and make decisions based on experience. Whether eating out or whether to use the stairs or the elevator, every day we make a decision. Sometimes we&rsquo;re doing what we know and sometimes we&rsquo;re winging it with something new. These are a form of reinforcement learning. This form of learning is at the heart of most living things; with infants learning to walk by making mistakes and elephants in a zoo learning that electric fence must be kept away from."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://mwauracollins.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making","item":"https://mwauracollins.github.io/posts/2025-02-23-rl_bandits/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making","name":"Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making","description":"Every day, we interact with our world and make decisions based on experience. Whether eating out or whether to use the stairs or the elevator, every day we make a decision. Sometimes we\u0026rsquo;re doing what we know and sometimes we\u0026rsquo;re winging it with something new. These are a form of reinforcement learning. This form of learning is at the heart of most living things; with infants learning to walk by making mistakes and elephants in a zoo learning that electric fence must be kept away from.\n","keywords":["Reinforcement Learning","Bandits","Mathy"],"articleBody":"Every day, we interact with our world and make decisions based on experience. Whether eating out or whether to use the stairs or the elevator, every day we make a decision. Sometimes we‚Äôre doing what we know and sometimes we‚Äôre winging it with something new. These are a form of reinforcement learning. This form of learning is at the heart of most living things; with infants learning to walk by making mistakes and elephants in a zoo learning that electric fence must be kept away from.\nEssentially, RL, is decision-making under uncertainty and trial-and-error learning. Be it a computer playing GO or robots learning to walk, reinforcement learning is what is behind these clever actions.\nRL is not some one monolithic chunk of high-level mathematics or some esoteric algorithms. It has some levels with elementary forms as we are going to discuss.\nThis article is part of the series in which I am going to work through RL from scratch to some concepts which you may have heard of such as GRPO.\nWhat This Series Covers We will start with bandit algorithms which are the the tutorial level of RL before covering the final bosses.\nSome of the concepts to be covered include:\nMulti-armed Bandits - Simple form of learning from rewards Markov Decision Processes (MDPs) - The foundation of RL; The math behind decision making over time. Dynamic Programming and Monte Carlo Methods - Smarter ways to solve MDPs without brute force Temporal Difference Learning - Learning on the fly Policy Gradient Methods - Teaching how to act and not what actions give you sugars Actor-Critic - Making everything smoother and stable Multi Armed Bandits For this, I am taking you to VEGASüéâ. Imagine you walk into a casino and your goal is to maximize your winnings by figuring out which machine is the jackpot. What is your approach? Do you just stick with one machine that gives you some money? Or do you try out different machines hoping you might be blessed by the gods of luck?\nThat‚Äôs the multi-armed bandit problem ‚Äî a fundamental dilemma between exploration and exploitation. And believe it or not, understanding this simple problem sets the stage for everything else in RL.\nLet‚Äôs denote some notations which are fundamental to Multi-Armed Bandit problems\nNotation $k$: number of actions (arms)\n$t$: discrete time step\n$q^*(a)$: true value of action $a$\n$Q_t(a)$: estimate of $q^*(a)$ at time $t$\n$N_t(a)$: number of selections of action $a$ up to time $t$\n$H_t(a)$: preference for action $a$ at time $t$\n$\\pi_t(a)$: probability of selecting action $a$ at time $t$\n$\\bar{R}_t$: estimated expected reward at time $t$\nDon‚Äôt be afraid of this. We are still in Vegas remember. I will guide you through everything. Now, let‚Äôs break down what each term means in the context of our casino analogy.\nStarting with $k$, this can be understood as the number of machines/arms each with an unknown reward distribution. $t$ represents the current decision round. Every time you pull a lever, $t$ increases by 1. $q^*(a)$ is the expected reward for pulling arm $a$, i.e., the true average payout of that machine over an infinite number of pulls. Remember the true action value is not a fixed amount but an expectation over many trials. Some machines might have high variance (big payouts rarely, small payouts often). $Q_t(a)$ is your best estimate the true value of a machine $q^*(a)$ because you‚Äôve never been to Vegas and therefore you don‚Äôt know which machine is has better payouts. $\\bar{R}_t$ is the sample mean reward observed up to time $t$. If we‚Äôve chosen different machines, this is the overall average reward across all actions taken so far..\nBack to the question, do you explore $k$ machines or do you just stick to one? If you choose to exploit one, you may miss out on a better machine while if you decide to explore the options, you may lose some money but in the long run have an understanding on which machines are the best.\nBut how do we estimate the value of an action?\nAction-Value Methods We will start by understanding how we can estimate the value of the action we have taken at the currect timestep.\nA simple way to define the estimate is by averaging past rewards we have gotten from choosing the action.\n$$ Q_t(a) = \\frac{\\text{sum of rewards when a taken prior to t}}{\\text{number of times a taken prior to t}} $$This might not be the best approach to estimate the value of an action but at it is a reasonable starting point.\nGreedy Action Selection Now that we have estimates $Q_t(a)$, we can choose actions based on these estimates. A straightforward way to make decisions is to always pick the action with the highest estimated value:\n$$ A_t = \\arg\\max_a Q_t(a) $$This means at each step, we choose the action that has historically given the highest rewards. This is called the greedy policy, because we are always exploiting what we know.\nIf we only exploit, we might get stuck on a suboptimal action. What if there‚Äôs a better action we haven‚Äôt tried enough times? This is why we introduce exploration.\n$\\varepsilon$-Greedy Exploration Instead of always choosing the greedy action, a better approach is to sometimes try other actions. One way to do this is the Œµ-greedy method, where we:\nWith probability $1 - \\varepsilon$, pick the best-known action (exploit). With probability $\\varepsilon$, pick a random action (explore).\nMathematically:\n$$ \\pi_t(a) = \\begin{cases} 1 - \\varepsilon + \\frac{\\varepsilon}{k}, \u0026 \\text{if } a = \\arg\\max Q_t(a) \\\\ \\frac{\\varepsilon}{k}, \u0026 \\text{otherwise} \\end{cases} $$This is better because:\nIt prevents getting stuck on suboptimal actions. It gradually improves estimates of less-explored actions. It balances short-term gains with long-term learning. Incremental Implementation The problem with estimating the value of actions through averaging is that you need to keep count of all the rewards gotten from past actions. This is inefficient considering cases when the timesteps approaches infinity.\n$$ \\begin{aligned} Q_n(a) \u0026= \\frac{R_1 + R_2 + \\cdots + R_{n-1}}{n-1} = \\color{blue} \\frac{1}{n-1} \\sum_{i=1}^{n-1} R_i \\\\ Q_{n+1}(a) \u0026= \\frac{R_1 + R_2 + \\cdots + R_n}{n} = \\frac{1}{n} \\sum_{i=1}^{n} R_i \\\\ \u0026= \\frac{1}{n} \\left(R_n + \\sum_{i=1}^{n-1} R_i\\right) \\\\ \u0026= \\frac{1}{n} \\left(R_n + (n-1) \\color{blue} \\frac{1}{n-1} \\sum_{i=1}^{n-1} R_i \\color{none} \\right) \\\\ \u0026= \\frac{1}{n} R_n + \\frac{n-1}{n} Q_n \\\\ \u0026= \\frac{R_n + (n-1)Q_n}{n} \\\\ \u0026= \\frac{R_n + nQ_n - Q_n}{n} \\\\ \u0026= Q_n + \\frac{1}{n}({R_n - Q_n}) \\end{aligned} $$From the derived equations, we can observe that now the value can be estimated with just the initial value and the last reward.\nThe general form of this equation is and it looks similar to the update rule of gradient ascent:\n$$ \\text{NewEstimate} \\leftarrow \\text{OldEstimate} + \\text{StepSize} \\cdot (\\text{Target} - \\text{OldEstimate}) $$Handling Non-Stationary Problems So far, we have been treating bandits as if the reward distribution are fixed, like the slot machines have the same payout probabilities. But is that really true? Machines that used to pay, may suddenly turn stingy.\nThis is the case for non-stationary environments, where the true value of actions changes as time progresses and if we treat past data as equally relevant, our estimates will lag behind reality.\nOur previous update equation;\n$$ Q_{n+1}(a) = Q_n + \\frac{1}{n}({R_n - Q_n}) $$relies on all past rewards equally. However, if the reward distribution shifts, this approach is too slow to adapt since older data still heavily influences $Q_{n+1}$. A better approach is to gradually ‚Äúforget‚Äù old data by giving recent rewards more importance.\nInstead of using $\\frac{1}{n}$ as the step size parameter, we will use a fixed step-size parameter $\\alpha$ where $0 \u003c \\alpha \\leq 1$.\nThis update rule ensures that the learning is slow and stable if $\\alpha$ is small or fast and responsive if $\\alpha$ is large.\n$$ \\begin{aligned} Q_{n+1} \u0026= Q_n + \\alpha (R_n - Q_n) \\\\ \u0026= \\alpha R_n + (1-\\alpha) \\color{cyan} Q_n \\\\ \u0026= \\alpha R_n + (1-\\alpha) \\color{cyan} [\\alpha R_{n-1} + (1-\\alpha) Q_{n-1}] \\\\ \u0026= \\alpha R_n + \\alpha (1-\\alpha) R_{n-1} + (1-\\alpha)^2 \\color {yellow} Q_{n-1} \\\\ \u0026= \\alpha R_n + \\alpha (1-\\alpha) R_{n-1} + (1-\\alpha)^2 \\color{yellow} [\\alpha R_{n-2} +(1 - \\alpha)Q_{n-2}] \\\\ \u0026\\\\ \u0026\\text{Continuing the expansion of } Q: \\\\ \u0026\\\\ \u0026= \\alpha R_n + \\alpha (1-\\alpha) R_{n-1} + \\alpha (1-\\alpha)^2 R_{n-2} + \\cdots + \\alpha (1-\\alpha)^{n-1} R_1 + (1-\\alpha)^n Q_1 \\\\ \u0026= (1-\\alpha)^n Q_1 + \\sum_{i=1}^{n} \\alpha (1-\\alpha)^{n-i} R_i \\end{aligned} $$Woah woah woah. Let take in what is happening.\n$Q_{n+1}$ is the estimate of the action value at the next time step. $(1-\\alpha)^n Q_1$ is the influence of the initial estimate of the action value before any rewards are observed. The factor $(1-\\alpha)$ determines how much the initial estimate still influences the current estimate. If $\\alpha$ is small, the influence slowly fades slowly and the opposite. $\\sum_{i=1}^{n} \\alpha (1-\\alpha)^{n-i} R_i$ sums up all previous rewards $R_i$ but weighs them differently. The weight of each past reward depends on the factor $ (1-\\alpha)^{n-i}$ which decays exponentially over time. Meaning, at infinity past reward will be near zero and have little effect. Only the most recent rewards matter.\nThis is known as the exponential moving average or exponential recency-weighted average.\nUsing $\\frac{1}{n}$ as or step size in the first approach converges to $q^*(a)$ by the law of large numbers but reacts slowly to changes in non-stationary environments while using a fixed step size $\\alpha$ adats faster but never fully converges.\nCan we dynamically adjust the step size to get the best of both worlds?\nSome adaptive step-size strategies include:\nHarmonic Step-Size (Slower Decay) - $ \\alpha_n(a) = \\frac{c}{c + N_n(a)}$ Recency-Weighted Average (Exponential Decay) - $ \\alpha_n(a) = \\frac{1}{N_n(a)^\\beta}, \\space 0 \u003c \\beta \u003c 1 $ Optimistic Step-Size Adjustment - $ \\alpha_n(a) = \\frac{\\text{variance of rewards for a}}{\\text{number of times a was chosen}}$ Step-Size Type Converges? Adapts Quickly? Best for $ \\alpha_n = \\frac{1}{N_n(a)} $ (sample average) ‚úÖ Yes ‚ùå No Stationary problems $ \\alpha $ (constant step size) ‚ùå No ‚úÖ Yes Non-stationary problems $ \\alpha_n = \\frac{c}{c + N_n(a)} $ ‚úÖ Yes ‚úÖ Moderate Partially changing environments $ \\alpha_n = \\frac{1}{N_n^\\beta(a)}, \\quad 0 \u003c \\beta \u003c 1 $ ‚úÖ Yes ‚úÖ Good General RL settings Variance-based step-size ‚úÖ Yes ‚úÖ Best Noisy rewards Upper Confidence Bounds We have found out that random exploration with probability $\\varepsilon$ is great because it lets us try options which may be better than the currect action. However, we can end up exploring a bad action which we already know is poor. It is like someone knowing that a certain slot machine is not a money spitter and still trying it yet there are some machines that have not been tried out yet.\nInstead of just picking the one with the highest estimated value, what if we also consider how uncertain we are?\nFor an action $a$, at time $t$, the UCB selection rule is:\n$$ A_t = \\arg\\max_a \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right] $$where $c$ is the exploration parameter (higher values encourage more exploration)\nThis means that if an action has a high estimated reward, it gets chosen (exploitation) and if an action has been rarely tried i.e., $N_t(a)$ is small, the confidence term is large and we are more likely to explore it. Over time, the uncertainly decreases and we settle on the best action.\nThompson Sampling We saw that UCB selects actions by adding a confidence term to the action-value estimate but it only considers the worst-case uncertainty (favouring actions with fewer samples). It is a rule based approach.\nThompson Sampling takes a Bayesian approach‚Äîinstead of just using a point estimate of reward, we model a probability distribution for each action‚Äôs value and sample from it. Instead od relying on a single estimate of $Q_t(a)$, we assume that each action has a probability distribution for its reward which gets updated as we gather more data.\nWe start with a prior distribution over the expected rewards of every action, then for each action we sample a random reward from its estimated distribution. From there, we pick the action with t he highest sampled reward and then observe the actual reward and update the distribution.\nThis Bayesian approach naturally balances exploration and exploitation and it works better than $\\varepsilon$-greedy and UCB in many cases\nGradient Bandits Back in Vegas, most gamblers would estimate how good each machine is and use that. But I will teach you the real high-roller playstyle. Instead of jumping from machine to machine, tracking raw winnings for each machine (which is tiresome), we cann assign a preference score to each machine. A machine that seems better, gets a higher score. Have a taste in machines! And this is a better approach since it is self-adjusting.\nWhat we have discusses so far is estimating action-values $Q_t(a)$ and we would like to shift to learning based of preferences.\nGradient-based bandits learn a policy directly, rather than estimating action values. We will use softmax function to compute action probabilities:\n$$ \\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_b{e^{H_t(b)}}} $$ where:\n$H_t(a)$ is the preference for action $a$ at time $t$. $\\pi_t(a)$ is the probability of selecting action $a$. We update action preferences based on reward like:\n$$ \\begin{align*} H_{t+1}(a) \u0026= H_t(a) + \\alpha (R_t - \\bar{R}_t) (1 - \\pi_t(a)) \\\\ H_{t+1}(b) \u0026= H_t(b) - \\alpha (R_t - \\bar{R}_t) \\pi_t(b) \\quad \\forall b \\neq a \\end{align*} $$where:\n$\\bar{R}_t$ is the average reward up to time $t$. Generallly, gradient bandits work better for non-stationary problems and they don‚Äôt require value estimation\n","wordCount":"2231","inLanguage":"en","datePublished":"2025-02-23T22:30:36+03:00","dateModified":"2025-02-23T22:30:36+03:00","author":{"@type":"Person","name":"Mwaura Collins"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mwauracollins.github.io/posts/2025-02-23-rl_bandits/"},"publisher":{"@type":"Organization","name":"mwaura.AI","logo":{"@type":"ImageObject","url":"https://mwauracollins.github.io/favicon.ico"}}}</script><script>MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]]}}</script><script defer src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mwauracollins.github.io/ accesskey=h title="mwaura.AI (Alt + H)">mwaura.AI</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mwauracollins.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://mwauracollins.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://mwaura.tech/ title=mwaura.tech><span>mwaura.tech</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mwauracollins.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://mwauracollins.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making</h1><div class=post-meta><span title='2025-02-23 22:30:36 +0300 EAT'>February 23, 2025</span>&nbsp;¬∑&nbsp;11 min&nbsp;¬∑&nbsp;2231 words&nbsp;¬∑&nbsp;Mwaura Collins</div></header><div class=post-content><p>Every day, we interact with our world and make decisions based on experience. Whether eating out or whether to use the stairs or the elevator, every day we make a decision. Sometimes we&rsquo;re doing what we know and sometimes we&rsquo;re winging it with something new. These are a form of reinforcement learning. This form of learning is at the heart of most living things; with infants learning to walk by making mistakes and elephants in a zoo learning that electric fence must be kept away from.</p><p>Essentially, RL, is decision-making under uncertainty and trial-and-error learning. Be it a computer playing GO or robots learning to walk, reinforcement learning is what is behind these clever actions.</p><p>RL is not some one monolithic chunk of high-level mathematics or some esoteric algorithms. It has some levels with elementary forms as we are going to discuss.</p><p>This article is part of the series in which I am going to work through RL from scratch to some concepts which you may have heard of such as GRPO.</p><h2 id=what-this-series-covers>What This Series Covers<a hidden class=anchor aria-hidden=true href=#what-this-series-covers>#</a></h2><p>We will start with <strong>bandit algorithms</strong> which are the the tutorial level of RL before covering the final bosses.</p><p>Some of the concepts to be covered include:</p><ul><li><a href=#multi-armed-bandits>Multi-armed Bandits</a> - Simple form of learning from rewards</li><li>Markov Decision Processes (MDPs) - The foundation of RL; The math behind decision making over time.</li><li>Dynamic Programming and Monte Carlo Methods - Smarter ways to solve MDPs without brute force</li><li>Temporal Difference Learning - Learning on the fly</li><li>Policy Gradient Methods - Teaching how to act and not what actions give you sugars</li><li>Actor-Critic - Making everything smoother and stable</li></ul><h2 id=multi-armed-bandits>Multi Armed Bandits<a hidden class=anchor aria-hidden=true href=#multi-armed-bandits>#</a></h2><hr><p>For this, I am taking you to VEGASüéâ. Imagine you walk into a casino and your goal is to maximize your winnings by figuring out which machine is the jackpot. What is your approach? Do you just stick with one machine that gives you some money? Or do you try out different machines hoping you <em>might</em> be blessed by the gods of luck?</p><hr><p>That‚Äôs the multi-armed bandit problem ‚Äî a fundamental dilemma between exploration and exploitation. And believe it or not, understanding this simple problem sets the stage for everything else in RL.</p><p>Let&rsquo;s denote some notations which are fundamental to Multi-Armed Bandit problems</p><h2 id=notation>Notation<a hidden class=anchor aria-hidden=true href=#notation>#</a></h2><p>$k$: number of actions (arms)</p><p>$t$: discrete time step</p><p>$q^*(a)$: true value of action $a$</p><p>$Q_t(a)$: estimate of $q^*(a)$ at time $t$</p><p>$N_t(a)$: number of selections of action $a$ up to time $t$</p><p>$H_t(a)$: preference for action $a$ at time $t$</p><p>$\pi_t(a)$: probability of selecting action $a$ at time $t$</p><p>$\bar{R}_t$: estimated expected reward at time $t$</p><p>Don&rsquo;t be afraid of this. We are still in Vegas remember. I will guide you through everything. Now, let‚Äôs break down what each term means in the context of our casino analogy.</p><p>Starting with $k$, this can be understood as the number of machines/arms each with an unknown reward distribution. $t$ represents the current decision round. Every time you pull a lever, $t$ increases by 1. $q^*(a)$ is the expected reward for pulling arm $a$, i.e., the true average payout of that machine over an infinite number of pulls. Remember the true action value is not a fixed amount but an expectation over many trials. Some machines might have high variance (big payouts rarely, small payouts often). $Q_t(a)$ is your best estimate the true value of a machine $q^*(a)$ because you&rsquo;ve never been to Vegas and therefore you don&rsquo;t know which machine is has better payouts. $\bar{R}_t$ is the sample mean reward observed up to time $t$. If we‚Äôve chosen different machines, this is the overall average reward across all actions taken so far..</p><p>Back to the question, do you explore $k$ machines or do you just stick to one? If you choose to <strong>exploit</strong> one, you may miss out on a better machine while if you decide to <strong>explore</strong> the options, you may lose some money but in the long run have an understanding on which machines are the best.</p><p>But how do we estimate the value of an action?</p><h3 id=action-value-methods>Action-Value Methods<a hidden class=anchor aria-hidden=true href=#action-value-methods>#</a></h3><p>We will start by understanding how we can estimate the value of the action we have taken at the currect timestep.</p><p>A simple way to define the estimate is by averaging past rewards we have gotten from choosing the action.</p>$$
Q_t(a) = \frac{\text{sum of rewards when a taken prior to t}}{\text{number of times a taken prior to t}}
$$<p>This might not be the best approach to estimate the value of an action but at it is a reasonable starting point.</p><h3 id=greedy-action-selection>Greedy Action Selection<a hidden class=anchor aria-hidden=true href=#greedy-action-selection>#</a></h3><p>Now that we have estimates $Q_t(a)$, we can choose actions based on these estimates. A straightforward way to make decisions is to always pick the action with the highest estimated value:</p>$$
A_t = \arg\max_a Q_t(a)
$$<p>This means at each step, we choose the action that has historically given the highest rewards. This is called the <strong>greedy</strong> policy, because we are always exploiting what we know.</p><p>If we only exploit, we might get stuck on a suboptimal action. What if there‚Äôs a better action we haven‚Äôt tried enough times? This is why we introduce <strong>exploration</strong>.</p><h3 id=-greedy-exploration>$\varepsilon$-Greedy Exploration<a hidden class=anchor aria-hidden=true href=#-greedy-exploration>#</a></h3><p>Instead of always choosing the greedy action, a better approach is to sometimes try other actions. One way to do this is the Œµ-greedy method, where we:</p><p>With probability $1 - \varepsilon$, pick the best-known action (exploit).
With probability $\varepsilon$, pick a random action (explore).</p><p>Mathematically:</p>$$
\pi_t(a) = \begin{cases}
1 - \varepsilon + \frac{\varepsilon}{k}, & \text{if } a = \arg\max Q_t(a) \\
\frac{\varepsilon}{k}, & \text{otherwise}
\end{cases}
$$<p>This is better because:</p><ul><li>It prevents getting stuck on suboptimal actions.</li><li>It gradually improves estimates of less-explored actions.</li><li>It balances short-term gains with long-term learning.</li></ul><h2 id=incremental-implementation>Incremental Implementation<a hidden class=anchor aria-hidden=true href=#incremental-implementation>#</a></h2><p>The problem with estimating the value of actions through averaging is that you need to keep count of all the rewards gotten from past actions. This is inefficient considering cases when the timesteps approaches infinity.</p>$$
\begin{aligned}
Q_n(a) &= \frac{R_1 + R_2 + \cdots + R_{n-1}}{n-1} = \color{blue} \frac{1}{n-1} \sum_{i=1}^{n-1} R_i \\
Q_{n+1}(a) &= \frac{R_1 + R_2 + \cdots + R_n}{n} = \frac{1}{n} \sum_{i=1}^{n} R_i \\
&= \frac{1}{n} \left(R_n + \sum_{i=1}^{n-1} R_i\right) \\
&= \frac{1}{n} \left(R_n + (n-1) \color{blue} \frac{1}{n-1} \sum_{i=1}^{n-1} R_i \color{none} \right) \\
&= \frac{1}{n} R_n + \frac{n-1}{n} Q_n \\
&= \frac{R_n + (n-1)Q_n}{n} \\
&= \frac{R_n + nQ_n - Q_n}{n} \\
&= Q_n + \frac{1}{n}({R_n - Q_n})
\end{aligned}
$$<p>From the derived equations, we can observe that now the value can be estimated with just the initial value and the last reward.<br>The general form of this equation is and it looks similar to the update rule of gradient ascent:</p>$$
\text{NewEstimate} \leftarrow \text{OldEstimate} + \text{StepSize} \cdot (\text{Target} - \text{OldEstimate})
$$<h2 id=handling-non-stationary-problems>Handling Non-Stationary Problems<a hidden class=anchor aria-hidden=true href=#handling-non-stationary-problems>#</a></h2><p>So far, we have been treating bandits as if the reward distribution are fixed, like the slot machines have the same payout probabilities. But is that really true? Machines that used to pay, may suddenly turn stingy.</p><p>This is the case for non-stationary environments, where the true value of actions changes as time progresses and if we treat past data as equally relevant, our estimates will lag behind reality.</p><p>Our previous update equation;</p>$$
Q_{n+1}(a) = Q_n + \frac{1}{n}({R_n - Q_n})
$$<p>relies on all past rewards equally. However, if the reward distribution shifts, this approach is too slow to adapt since older data still heavily influences $Q_{n+1}$. A better approach is to gradually &ldquo;forget&rdquo; old data by giving recent rewards more importance.</p><p>Instead of using $\frac{1}{n}$ as the step size parameter, we will use a fixed step-size parameter $\alpha$ where $0 < \alpha \leq 1$.</p><p>This update rule ensures that the learning is slow and stable if $\alpha$ is small or fast and responsive if $\alpha$ is large.</p>$$
\begin{aligned}
Q_{n+1} &= Q_n + \alpha (R_n - Q_n) \\
&= \alpha R_n + (1-\alpha) \color{cyan} Q_n \\
&= \alpha R_n + (1-\alpha) \color{cyan} [\alpha R_{n-1} + (1-\alpha) Q_{n-1}] \\
&= \alpha R_n + \alpha (1-\alpha) R_{n-1} + (1-\alpha)^2 \color {yellow} Q_{n-1} \\
&= \alpha R_n + \alpha (1-\alpha) R_{n-1} + (1-\alpha)^2 \color{yellow} [\alpha R_{n-2} +(1 - \alpha)Q_{n-2}] \\
&\\
&\text{Continuing the expansion of } Q: \\
&\\
&= \alpha R_n + \alpha (1-\alpha) R_{n-1} + \alpha (1-\alpha)^2 R_{n-2} + \cdots + \alpha (1-\alpha)^{n-1} R_1 + (1-\alpha)^n Q_1 \\
&= (1-\alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1-\alpha)^{n-i} R_i
\end{aligned}
$$<p>Woah woah woah. Let take in what is happening.</p><p>$Q_{n+1}$ is the estimate of the action value at the next time step. $(1-\alpha)^n Q_1$ is the influence of the initial estimate of the action value before any rewards are observed. The factor $(1-\alpha)$ determines how much the initial estimate still influences the current estimate. If $\alpha$ is small, the influence slowly fades slowly and the opposite. $\sum_{i=1}^{n} \alpha (1-\alpha)^{n-i} R_i$ sums up all previous rewards $R_i$ but weighs them differently. The weight of each past reward depends on the factor $ (1-\alpha)^{n-i}$ which decays exponentially over time. Meaning, at infinity past reward will be near zero and have little effect. Only the most recent rewards matter.</p><p>This is known as the <strong>exponential moving average</strong> or <strong>exponential recency-weighted average</strong>.</p><hr><p>Using $\frac{1}{n}$ as or step size in the <a href=#action-value-methods>first approach</a> converges to $q^*(a)$ by the law of large numbers but reacts slowly to changes in non-stationary environments while using a fixed step size $\alpha$ adats faster but never fully converges.</p><p>Can we dynamically adjust the step size to get the best of both worlds?</p><p>Some adaptive step-size strategies include:</p><ol><li>Harmonic Step-Size (Slower Decay) - $ \alpha_n(a) = \frac{c}{c + N_n(a)}$</li><li>Recency-Weighted Average (Exponential Decay) - $ \alpha_n(a) = \frac{1}{N_n(a)^\beta}, \space 0 < \beta < 1 $</li><li>Optimistic Step-Size Adjustment - $ \alpha_n(a) = \frac{\text{variance of rewards for a}}{\text{number of times a was chosen}}$</li></ol><table><thead><tr><th>Step-Size Type</th><th>Converges?</th><th>Adapts Quickly?</th><th>Best for</th></tr></thead><tbody><tr><td>$ \alpha_n = \frac{1}{N_n(a)} $ (sample average)</td><td>‚úÖ Yes</td><td>‚ùå No</td><td>Stationary problems</td></tr><tr><td>$ \alpha $ (constant step size)</td><td>‚ùå No</td><td>‚úÖ Yes</td><td>Non-stationary problems</td></tr><tr><td>$ \alpha_n = \frac{c}{c + N_n(a)} $</td><td>‚úÖ Yes</td><td>‚úÖ Moderate</td><td>Partially changing environments</td></tr><tr><td>$ \alpha_n = \frac{1}{N_n^\beta(a)}, \quad 0 < \beta < 1 $</td><td>‚úÖ Yes</td><td>‚úÖ Good</td><td>General RL settings</td></tr><tr><td>Variance-based step-size</td><td>‚úÖ Yes</td><td>‚úÖ Best</td><td>Noisy rewards</td></tr></tbody></table><hr><h2 id=upper-confidence-bounds>Upper Confidence Bounds<a hidden class=anchor aria-hidden=true href=#upper-confidence-bounds>#</a></h2><p>We have found out that random exploration with probability $\varepsilon$ is great because it lets us try options which <em>may</em> be better than the currect action. However, we can end up exploring a bad action which we already know is poor. It is like someone knowing that a certain slot machine is not a money spitter and still trying it yet there are some machines that have not been tried out yet.</p><p>Instead of just picking the one with the highest estimated value, what if we also consider how uncertain we are?</p><p>For an action $a$, at time $t$, the UCB selection rule is:</p>$$
A_t = \arg\max_a \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]
$$<p>where $c$ is the exploration parameter (higher values encourage more exploration)</p><p>This means that if an action has a high estimated reward, it gets chosen (exploitation) and if an action has been rarely tried i.e., $N_t(a)$ is small, the confidence term is large and we are more likely to explore it. Over time, the uncertainly decreases and we settle on the best action.</p><h2 id=thompson-sampling>Thompson Sampling<a hidden class=anchor aria-hidden=true href=#thompson-sampling>#</a></h2><p>We saw that UCB selects actions by adding a confidence term to the action-value estimate but it only considers the worst-case uncertainty (favouring actions with fewer samples). It is a rule based approach.</p><p>Thompson Sampling takes a Bayesian approach‚Äîinstead of just using a point estimate of reward, we model a probability distribution for each action‚Äôs value and sample from it. Instead od relying on a single estimate of $Q_t(a)$, we assume that each action has a probability distribution for its reward which gets updated as we gather more data.</p><p>We start with a prior distribution over the expected rewards of every action, then for each action we sample a random reward from its estimated distribution. From there, we pick the action with t he highest sampled reward and then observe the actual reward and update the distribution.</p><p>This Bayesian approach naturally balances exploration and exploitation and it works better than $\varepsilon$-greedy and UCB in many cases</p><h2 id=gradient-bandits>Gradient Bandits<a hidden class=anchor aria-hidden=true href=#gradient-bandits>#</a></h2><hr><p>Back in Vegas, most gamblers would estimate how good each machine is and use that. But I will teach you the real high-roller playstyle. Instead of jumping from machine to machine, tracking raw winnings for each machine (which is tiresome), we cann assign a preference score to each machine.
A machine that seems better, gets a higher score. Have a taste in machines! And this is a better approach since it is self-adjusting.</p><hr><p>What we have discusses so far is estimating action-values $Q_t(a)$ and we would like to shift to learning based of preferences.</p><p>Gradient-based bandits learn a policy directly, rather than estimating action values. We will use softmax function to compute action probabilities:</p>$$
\pi_t(a) = \frac{e^{H_t(a)}}{\sum_b{e^{H_t(b)}}}
$$<p>where:</p><ul><li>$H_t(a)$ is the preference for action $a$ at time $t$.</li><li>$\pi_t(a)$ is the probability of selecting action $a$.</li></ul><p>We update action preferences based on reward like:</p>$$
\begin{align*}
H_{t+1}(a) &= H_t(a) + \alpha (R_t - \bar{R}_t) (1 - \pi_t(a)) \\
H_{t+1}(b) &= H_t(b) - \alpha (R_t - \bar{R}_t) \pi_t(b) \quad \forall b \neq a
\end{align*}
$$<p>where:</p><ul><li>$\bar{R}_t$ is the average reward up to time $t$.</li></ul><p>Generallly, gradient bandits work better for non-stationary problems and they don&rsquo;t require value estimation</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://mwauracollins.github.io/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=https://mwauracollins.github.io/tags/bandits/>Bandits</a></li><li><a href=https://mwauracollins.github.io/tags/mathy/>Mathy</a></li></ul><nav class=paginav><a class=next href=https://mwauracollins.github.io/posts/2025-02-04-gpt/><span class=title>Next ¬ª</span><br><span>Rolling Your Own GPT</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making on x" href="https://x.com/intent/tweet/?text=Reinforcement%20Learning%3a%20From%20Bandits%20to%20GRPO%20%e2%80%93%20A%20Journey%20Through%20Decision-Making&amp;url=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-23-rl_bandits%2f&amp;hashtags=ReinforcementLearning%2cBandits%2cMathy"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-23-rl_bandits%2f&amp;title=Reinforcement%20Learning%3a%20From%20Bandits%20to%20GRPO%20%e2%80%93%20A%20Journey%20Through%20Decision-Making&amp;summary=Reinforcement%20Learning%3a%20From%20Bandits%20to%20GRPO%20%e2%80%93%20A%20Journey%20Through%20Decision-Making&amp;source=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-23-rl_bandits%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-23-rl_bandits%2f&title=Reinforcement%20Learning%3a%20From%20Bandits%20to%20GRPO%20%e2%80%93%20A%20Journey%20Through%20Decision-Making"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-23-rl_bandits%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making on whatsapp" href="https://api.whatsapp.com/send?text=Reinforcement%20Learning%3a%20From%20Bandits%20to%20GRPO%20%e2%80%93%20A%20Journey%20Through%20Decision-Making%20-%20https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-23-rl_bandits%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making on telegram" href="https://telegram.me/share/url?text=Reinforcement%20Learning%3a%20From%20Bandits%20to%20GRPO%20%e2%80%93%20A%20Journey%20Through%20Decision-Making&amp;url=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-23-rl_bandits%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning: From Bandits to GRPO ‚Äì A Journey Through Decision-Making on ycombinator" href="https://news.ycombinator.com/submitlink?t=Reinforcement%20Learning%3a%20From%20Bandits%20to%20GRPO%20%e2%80%93%20A%20Journey%20Through%20Decision-Making&u=https%3a%2f%2fmwauracollins.github.io%2fposts%2f2025-02-23-rl_bandits%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://mwauracollins.github.io/>mwaura.AI</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>